[{"title":"JVM垃圾回收入门","date":"2021-09-11T06:20:09.586Z","url":"/WindShadow/JVM/Garbage-Collection/JVM%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%85%A5%E9%97%A8/","categories":[["JVM","/categories/JVM/"],["Garbage Collection","/categories/JVM/Garbage-Collection/"]],"content":"JVM垃圾回收入门本文JVM以常用的Hotspot虚拟机为例。 JVM内存分区Java垃圾回收也叫GC（Garbage Collection），了解GC之前先了解JVM内存分区。 JVM将内存分为5大部分：方法区、堆、虚拟机栈、程序计数器、本地方法栈 如图（图片来自互联网） 垃圾回就是释放垃圾占用的空间，防止内存泄露。有效的使用可以使用的内存，Java中主要对内存堆（Heap）中已经死亡的或者长时间没有使用的对象进行清除和回收。 下文《分代回收模型与分代回收算法》小节着重讲方法区和堆，此处先照个面。 什么是垃圾知道垃圾回收大体概念后，那么什么是垃圾呢？或者说什么样的对象才能称之为垃圾？ 定义：没有任何引用指向的一个对象或多个对象（循环引用，见下文） 如何定位垃圾引用计数法（reference count）给每个对象添加一个计数器，当有地方引用该对象时计数器加1，当引用失效时计数器减1。用对象计数器是否为0来判断对象是否可被回收。 缺点：无法解决循环引用。 显然循环引用的多个对象也要被认为是垃圾，但是引用计数法无法定位到它们。 根可达算法（roots searching）顾名思义，一种以某种对象为起点的搜索算法，可以被搜索到（可达性）的对象就不认为是垃圾，反之不可达对象就是垃圾，这种作为起点的对象称之为GC Roots。 GC Roots主要有：线程栈变量、静态变量、常量池、JNI指针等 垃圾回收算法标记清除（Mark - Sweep）优点：快 缺点：可用内存位置不连续，产生碎片 复制算法（Copying）优点：不产生内存碎片 缺点：浪费空间（8G的内存实际只能用4G） 标记压缩（Mark - Compact）标记清除之后再进行一次压缩。 缺点：效率偏低 分代回收模型与分代回收算法JVM方法区（Method Area）JVM方法区中存在roots searching算法所需的GC Roots，所以还是要提一提。 方法区只是一个逻辑概念（jvm规范中有言），对应1.7的永久代和1.8的元空间 永久代和元空间： 永久代必须指定大小限制，元空间可设置也可不设置，无上限（受限于物理内存） 字符串常量1.7时存在于永久代，1.8后存在于堆中 JVM堆（Heap）JVM将堆内存区域分成新生代 + 老年代，各区域默认比例如图所示（可通过启动参数配置） Young GC、Full GC在新生代触发的GC叫Young GC，即YGC，也叫Minor GC，在老年代触发的GC叫Full GC，即FGC，也叫Major GC。 Eden区大多数情况下，对象会在新生代Eden区中分配，当Eden区没有足够空间进行分配时，虚拟机会发起一次YGC。 Survivor区Survivor区相当于是Eden区和Tenured区的一个缓冲区，Survivor区又分为2个区，一个是From区，一个是To区。每次执行YGC时，会将Eden区中存活的对象放到Survivor的From区，在From区中仍存活的对象会移动到To区。From区和To区的逻辑关系会发生交换： From变To，To变From，目的是保证有连续的空间存放对方，避免内存碎片化。 Tenured区（老年代）JVM给每个对象定义了一个对象年龄（Age）计数器； 如果对象在Eden出生并经过第一次YGC后仍然存活，并且能被Survivor容纳的话，将被移动到Survivor空间中，并且对象年龄设为1。对象在Survivor区中每经历一次YGC（正常情况下，对象经历YGC后存活下来的时，会不断在Survivor的From与To区之间移动），年龄就增加1岁，当它的年龄增加到阈值（默认15岁，可以通过参数 XX:MaxPretenuringThreshold 设置 ），就将会晋升到老年代中。 对象内存分配规则大多数情况下，对象会在新生代Eden区中分配，当Eden区没有足够空间进行分配时，虚拟机会发起一次YGC。YGC相比FGC更频繁，回收速度也更快。而较大的对象（需要大量连续内存空间的Java对象 ）直接进入老年代。 通过YGC之后，Eden区中绝大部分对象会被回收，而那些存活对象，将会送到Survivor的From区，若From区空间不够，则直接进入老年代 。 分代回收算法分代回收算法是融合上述3种基础垃圾回收算法的思想，而产生针对分代回收模型中不同代采用不同算法的一套组合拳。 在新生代中，每次YGC都会有大批对象死去，少量对象存活，选用复制算法（Copying）来回收垃圾，只需复制出少量存活对象就可完成收集。复制成本低，效率高。 在老年代触发的FGC，虽然触发了GC，但是对象存活率高，没有额外空间对它进行分配担保，一般使用标记压缩（Mark - Compact）来进行回收，保证可用空间的连续性。 "},{"title":"何为Java线程中断","date":"2021-09-03T07:28:43.518Z","url":"/WindShadow/Java%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E4%BD%95%E4%B8%BAJava%E7%BA%BF%E7%A8%8B%E4%B8%AD%E6%96%AD/","categories":[["Java多线程","/categories/Java%E5%A4%9A%E7%BA%BF%E7%A8%8B/"]],"content":"何为Java线程中断Java线程中断是一个大多熟人不太关心的知识点，有的人可能一辈子也用不到，这是一位网络上的大牛说的。 Java线程的中断其实非常简单，说白了就是一个中断标志位的改变。先来看Thread类3个关于中断的方法： interrupt()： 实例方法，中断该线程；意思就是把中断标志位改为true。 isInterrupted() 实例方法，测试该线程是否被中断；显然，根据中断标志位返回true或false。 Thread.interrupted() 静态方法，测试当前线程是否中断；该方法与isInterrupted()类似，只不过，如果线程发生了中断，则把中断标志清除，也就是改为false。 与中断相关的自然就是中断异常InterruptedException，当线程执行某些方法时不希望被中断，若被中断则抛出中断异常，如Thread.sleep()方法、Object#wait()方法。 中断线程sleep休眠 执行结果 中断线程因调用Object#wait()方法进入等待的状态 执行结果 如果不执行中断线程t，由于COMMON没有其它线程调用notify()方法或notifyAll()方法，线程 t 永远不会醒来。 我们知道，线程调用Object#wait()方法时，会释放该对象的锁，当其他线程拿到该对象的锁时，是否可以中断调用Object#wait()方法的线程？ 执行结果 可以看出，线程t 在等待状态时，虽然被中断，但此时 COMMON 对象的锁被线程2 持有，所以没有响应中断，当线程2 调用COMMON.notifyAll()方法，线程 1获得 COMMON 对象的锁，发现自己被中断了。 推测线程在调用Object#wait()方法时，并没有立即让出锁，此时锁还在自己手上，但允许其它线程过来抢，直到有其它线程拿走锁后，才真正的失去锁。 所以对于Object#wait()方法，线程只有在获得监视对象的锁时才能响应该方法的中断。 中断线程因调用LockSupport.park()方法进入等待的状态 执行结果 当线程调用LockSupport.park()使cpu对其不再进行调度时，可以通过中断的方式唤醒（当然LockSupport.unpark(Thread)也可以），继续执行该线程。 小结说了这么多，一般开发中线程中断用到的场景少之又少，更多的是出现在线程调度相关的操作中，如AQS，都是大佬们玩的。 似乎线程中断的设计就是为线程调度而设计的。开发中就算强行为了用而用，用其作为线程通信的手段，也会有更好的方案代替。"},{"title":"Java线程池原理（二）","date":"2021-08-28T09:30:13.294Z","url":"/WindShadow/Java%E5%A4%9A%E7%BA%BF%E7%A8%8B/Java%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%8E%9F%E7%90%86/Java%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%8E%9F%E7%90%86%EF%BC%88%E4%BA%8C%EF%BC%89/","categories":[["Java多线程","/categories/Java%E5%A4%9A%E7%BA%BF%E7%A8%8B/"],["Java线程池原理","/categories/Java%E5%A4%9A%E7%BA%BF%E7%A8%8B/Java%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%8E%9F%E7%90%86/"]],"content":"Java线程池原理（二）承接上文，我们知道了线程池可以复用线程去执行任务，Executors工具类通过设置不同的参数来得到一些特性的线程池，那么继续深入，了解线程池是如何实现线程复用、核心线程的维持、非核心线程的存活控制，这些核心功能整体是如何运作的。 线程复用、核心线程的维持、非核心线程的存活控制线程复用此小节重点讲线程池线程复用的机制，遇到一些影不易读懂的代码我们可以跳过。 首先，线程池executor方法是线程池的入口，顺着该方法分析，不难发现一个关键的方法：addWorker(Runnable firstTask, boolean core)，这个方法是负责添加Worker以执行任务的。 那么我们目的很明确，在此只需要关心3个点：1、具体添加了什么。2、任务怎么被执行的。3、firstTask和core参数怎么运用的。 ThreadPoolExecutor的addWorker方法这里我们先入为主一波： 猜测该方法添加了一个叫Worker的东西，Worker内部使用一个boolean变量标识其内部Thread的性质是核心还是非核心线程，按照我们的常规的编程思路确实容易这样想。 接下来分析addWorker方法。 总结：addWorker方法添加一个Worker到ThreadPoolExecutor保存worker的一个HashSet中，之后Worker内的Thread被启动。而firstTask参数作为Worker构造参数被使用，core参数仅参与了一个cas自旋操作，和Worker没啥关系。 显然，我们先入为主的想法并不正确。 前文讲到线程池内是通过Worker干活的，其内部有一个Thread在工作，Worker是线程池实现线程复用的关键。要了解线程池线程复用等原理势必要先了解Worker对象如何工作的。 WorkerWorker类是ThreadPoolExecutor的私有非静态内部类，继承了AQS且实现了Runnable接口，Java中的AQS在此不展开讲，到这里我们只需知道Worker继承了AQS使自身可对外提供加解锁的方法，外部的合理编码保证并发访问的安全性即可，即使不太了解AQS也没关系。通过Worker类的方法名也可大致知道是干什么的。 很显然，Worker内部的Thread运行时，会调用Worker自身的run方法，看看Worker的run方法做了哪些事。 ThreadPoolExecutor的runWorker方法来到ThreadPoolExecutor的runWorker方法，这个方法极其关键！！！ 到这里，线程复用的机制很明了了：通过循环不断从工作队列中获取任务并执行； 不难总结出Worker的工作机制：worker通过实现Runnable接口，内部维护一个Thread线程对象，将自身作为“跳板”，优先执行自身初始化时携带的任务，之后不断消费（执行）线程池工作队列中的任务，直到队列中任务没有任务为止，worker的任务就结束了。 用图表示ThreadPoolExecutor的runWorker方法主要的执行过程如下： 不过目前为止，我们还是没发现和核心线程与非核心线程相关的痕迹，带着疑问继续往下走。 核心线程的维持、非核心线程的存活控制阅读ThreadPoolExecutor的runWorker方法，不难发现，每个Worker对象结束使命后，都会走到ThreadPoolExecutor的processWorkerExit方法执行退出逻辑。该方法入参除了Worker对象以外还需要传入一个boolean类型的completedAbruptly参数，见明之意，该参数表示该Worker对象是否是“突然结束”，不难发现只有任务task执行期间抛出异常或error，Worker对象才算是“突然结束”，completedAbruptly为true，反之正常结束，completedAbruptly为false。 ThreadPoolExecutor的processWorkerExit方法综上所述，processWorkerExit方法职责就是处理Worker对象正常与非正常的结束，也可以理解为对Worker的回收。 小总结一下processWorkerExit主要干了几件事： 记录Worker对象完成的任务数到总记录中 移除该Worker对象 处理非正常结束Worker对象时，调整线程池记录的worker数量，补充一个新的worker且是以非核心线程的设定去补充的 提一下为什么Worker非正常结束时要补充新的Worker： 再仔细看runWorker方法，因为Worker非正常结束说明任务task的run方法抛出了异常（或error），尽管做了try-catch处理，作者的做法是把异常继续往抛出，而处理Worker的回收工作processWorkerExit方法是在finally块执行的，也就是说processWorkerExit方法执行完之后，异常就要被抛出去了，虚拟机接受到异常之后，就会销毁该线程，也就是销毁Worker内的Thread（线程死亡），此时需要补充新的Worker继续干活。 此处给读者留下一个小思考：既然runWorker方法catch到了异常或error，可以不抛出去吗？ 不过跟踪到这里还是没发现核心线程与非核心线程相关的操作，而且在补充新的worker时，还是以非核心线程去补充的，这也是疑问点，并且如果每个Worker使命结束后都要移除了，Worker内的线程也执行完代码也要结束生命了，说好的核心线程不会被销毁呢？ ThreadPoolExecutor的getTask方法回过头看，我们还差一个方法没有分析，那就是ThreadPoolExecutor的getTask方法，在runWorker方法中Worker通过getTask从工作队列中获取任务，不难得出：如果获取的任务为null，Worker将被回收。而工作队列是阻塞队列，所以如果Worker在获取任务时被阻塞，那么就不会被回收，从而实现Worker内Thread的“常驻”效果，这确实是个思路。话不多说进源码。 下列代码笔者将保留原生的官方英文注释，增加自己的注释。 这段代码比较短但是比较精妙，需要反复揣摩，下面贴出笔者制作的流程图 到此，终于看到线程池七八成左右的真身了：runWorker方法使worker不断执行任务和任务通知，processWorkerExit方法负责worker的回收工作，而getTask方法负责从工作队列获取任务进一步控制worker内Thread的阻塞等待，进一步控制worker的回收，从而实现核心线程的常驻效果和非核心线程的存活控制。 提交优先级与执行优先级有如下程序 问：提交到第几个任务（i = ?）时，线程池拒绝执行任务而抛出异常？ 答：i = 31时。 线程池可接受的最大任务数量为：最大线程数+工作队列容量=30。上述每个任务都进行了一次长时间睡眠，显然for循环体积任务执行完之前，线程池内没有一个任务执行完。 值得注意的是，控制台打印的“start run number: {n}”字符串，应该是110左右先被打印，其次是2130后被打印，最后是11~20区间的数字被打印，而不是我们生活中任务认为的“先提交就先得到执行”。 于是结合addWorker方法得出线程池提交优先级： 核心线程 &gt; 工作队列 &gt; 非核心线程 于是结合getTask方法得出线程池执行优先级： 核心线程 &gt; 非核心线程 &gt; 工作队列 小结通过以上4个方法的解读，我们可以重新认识worker对象： 通俗的说每个worker被创建之后，其使命就是不断从工作队列领取任务去执行，在无任务可执行（不是任务全部执行完毕，两者概念差别很大），即领取不到任务之后，就要被回收。 不仅如此，核心线程与非核心线程也要重新理解： 核心线程：worker对象内的线程在获取任务前，线程池状态处于running，此时若当前worker数（反应线程池内的线程数）小于等于设置的核心线程数，该线程以poll方式方式获取任务，若工作队列无任务，则该线程将一直等待新的任务提交进来，该线程也就顺理成章的成为线程池常驻线程，也就是核心线程 非核心线程：worker对象内的线程在获取任务前，线程池状态处于running，此时若当前worker数（反应线程池内的线程数）大于设置的核心线程数，该线程以take方式方式获取任务，若工作队列无任务，则该线程将等待新的任务提交进来，等待时长为keepAliveTime，超时之后便会被对应worker被回收，等待期间该线程也就成为线程池非核心线程。 所以每个worker内的线在启动工作期间都是相同的性质，不分核心与非核心，只有工作队列无任务可执行时，才能明确线程的性质是核心还是非核心，同时也决定了worker的回收与否（既分高下，也决生死？！）。 回过头来看addWorker方法中表示线程类型的core参数，也只是参与了当前worker数与核心线程数或非核心线程数的比较，再无他用，这也说的通了。 在阅读这些方法中，我们跳过了一些“影响阅读”的代码，如worker自身的lock和unlock究是为了防止啥，ThreadPoolExecutor成员变量中ReentrantLock的mainLock保证了什么的线程安全，以及线程池自身的其它功能，如线程池的worker数量和状态等如何感知的……请听下回合分解。"},{"title":"MySql索引详解","date":"2021-07-14T08:20:46.904Z","url":"/WindShadow/MySql/MySql%E7%B4%A2%E5%BC%95%E8%AF%A6%E8%A7%A3/","categories":[["MySql","/categories/MySql/"]],"content":"了解索引之前先了解一下约束。 约束（constraint）约束实际上就是表中数据的限制条件 ，表在设计的时候加入约束的目的就是为了保证表中的记录完整和有效 ，常见约束如下 非空约束（not null）约束的字段不能为null 唯一性约束（unique）约束的字段是唯一的（可以为null），Mysql会为该字段会自动创建索引。 唯一性约束也称列级约束。 主健约束（primary key(简称PK)）表中的某个字段添加主键约束后，该字段为主键字段，主键字段中出现的每一个数据都称为主键值，必须唯一且不能为null，即primary key = unique + not null。当一个字段同时被非空约束和唯一性约束时，自动成为主键。但oracle并不如此。 主键约束也称表级约束。 单一主键：给一个字段添加主键约束 复合主键：给多个字段联合添加一个主键约束(只能用表级定义)，用的少 外键约束（foreign key (简称FK)）若有两个表A、B，id是A的主键，而B中也有id字段，则id就是表B的外键，外键约束主要用来维护两个表之间数据的一致性。 某个字段添加外键约束之后，该字段称为外键字段，外键字段中每个数据都是外键值。 外键必须至少是受唯一性约束的。 外键值可以为null。 一张表可以有多个外键字段。 检查约束（check (mysql不支持，oracle支持））略 索引概念MySql官方对索引的定义为：索引（Index)）是帮助MySql高效获取数据的数据结构。 提取句子主干，就可以得到索引的本质：索引是数据结构。 在mysql当中，使用explain关键字查看一个SQL语句是否使用了索引进行检索 观察查询结果key列中是否列出了显示MySQL实际决定使用的键(索引) ，未使用则为null。 同时，索引是各种数据库进行优化的重要手段。 索引实现原理 在MySql当中，索引是一个单独的对象，不同的存储引擎以不同的形式存在。 在MyISAM存储引擎中，索引存储在一个.MYI文件中。 在InnoDB存储引擎中索引存储在表空间（tablespace）中。 在MEMORY存储引擎当中索引被存储在内存当中。 不管索引存储在哪里，索引在mysql当中都是一个B+树的形式存在。 （至于为啥MySql使用B+树作为存储的数据结构，一时半会也说不明白，笔者也是略知一二，不想在这强行装X，目前知道B+树的结构就好了） 索引类型普通索引是最基本的索引，它没有任何限制。 创建示例： 直接创建 修改表结构创建 创建表时创建 删除索引： 唯一索引与普通索引类似，不同之处在于：索引列的值必须唯一，但允许有空值，即列受唯一性约束（unique）；显然的，一张表可以有多个唯一索引。 创建唯一索引： 直接创建 修改表结构创建 创建表时创建 主键索引一种特殊的唯一索引。 主键不可重复，只能有一个列作为主键 一张表中主键索引只能存在一个 一般是在建表的时候同时创建主键索引： 组合（复合、联合）索引指多个字段上创建的索引 组合索引中，列值的组合必须唯一。 只有在查询条件中使用了创建索引时的第一个字段，索引才会被使用。 使用组合索引时遵循最左前缀集合 创建方式： 全文索引 InnoDB 存储引擎在MySql 5.6 之后才支持。 只有字段的数据类型为 char、varchar、text 及其系列才可以建全文索引。 跟普通索引稍有不同，使用全文索引的格式MATCH (columnName) AGAINST (&#39;string&#39;) match() 函数中指定的列必须和全文索引中指定的列完全相同，否则就会报错，无法使用全文索引，这是因为全文索引不会记录关键字来自哪一列。如果想要对某一列使用全文索引，需要单独为该列创建全文索引。 索引失效情况常见的索引失效情况： 以”%”开头进行模糊查询，索引失效（后缀有%时 不失效） 尽量避免模糊查询的时候以”%”开始,这是一种优化的手段/策略。 or 操作两边未同时使用索引，索引失效 如果使用or那么要求or两边的条件字段都要有索引，才会走索引，如果其中一边有一个字段没有索引，那么另一个字段上的索引也会失效。 使用组合（复合）索引时，未使用左侧的列，索引失效 遵循最左前缀集合。 数据类型出现隐式转化。如varchar不加单引号的话可能会自动转换为int型，使索引无效，产生全表扫描 在索引字段上使用not，&lt;&gt;，!=。不等于操作符是永远不会用到索引，对它的处理只会产生全表扫描。 优化方法： key&lt;&gt;0 改为 key&gt;0 or key&lt;0。很显然，索引是想知道你要什么而不是你不要什么，你品你细品。 索引的列参加了运算，索引失效 说实话，没人写这种sql吧。 索引的列使用了函数，索引失效 还有一点要注意的是：当全表扫描速度比索引速度快时，mysql会使用全表扫描，此时索引失效。 说白了就一句话：索引是想要你直接告诉它你想要什么，而不是你不要什么，也不是你把这个东西（字段）包装之后，说包装完就是这样子，给我找。┐(´-｀)┌ 回表mysql的索引分为两大类：聚簇索引、非聚簇索引，它们不是一种单独的索引类型，而是一种数据存储方式 聚簇索引将数据存储与索引放到了一块，找到索引也就找到了数据（严格来说这里的数据指的是行的物理位置），由于聚簇索引是将数据跟索引结构放到一块，因此一个表仅有一个聚簇索引。 如果表中没有定义主键，则第一个not NULL unique列是聚集索引。如果没有这样的索引，InnoDB 会隐式定义一个主键来作为聚簇索引。 下面是聚簇索引B+树的结构示意图 非聚簇索引非聚簇索引也叫辅助索引或二级索引，非聚簇索引叶子节点存储的不再是数据（行的物理位置），而是主键值 下面是非聚簇索引B+树的结构示意图 回表先通过非聚簇索引定位到主键值，在通过聚簇索引定位到行记录，这就是所谓的回表或回表查询，先定位主键值，再定位行记录，它的性能较扫一遍索引树更低。 索引覆盖MySql官网类似的说法是：使用explain查询计划时，explain的输出结果Extra字段为Using index时，能够触发索引覆盖。 也就是如果触发了索引覆盖，那么explain输出的Extra字段为Using index。 一般的说法是：索引中已经包含所有需要读取的列的查询称为覆盖索引。 就是利用优化器的优化机制，只搜索了非聚簇索引，不进行回表操作，然后达到select的目的。一般主要适用于select出的字段很少的情况。 举例： 有张user表（PK id,name, age, email, address，……），我们的模板是select name, age ，然后只需要建立一个name和age的复合索引，然后这个非聚簇索引的叶子节点有了这两个字段和主键字段（虽然主键在这里没啥用）。然后执行器只需要搜索这个二级索引即可，不需要回表操作就完成了select查询操作。 小结 主键上以及unique字段是上都会自动添加索引 虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表进行insert、update和delete时。因为更新表时，不仅要保存数据，还要保存一下索引文件。 建立索引会占用磁盘空间的索引文件。一般情况影响不大，但如果在一个大的表上创建了多种组合索引，索引文件大小会增长的很快。 索引只是提高效率的一个因素，如果有大数据量的表，就需要花时间研究建立最优的索引，或优化查询语句。 "},{"title":"MySql事务","date":"2021-07-14T07:24:30.795Z","url":"/WindShadow/MySql/MySql%E4%BA%8B%E5%8A%A1/","categories":[["MySql","/categories/MySql/"]],"content":"前提摘要：MySql只有使用InnoDB 引擎时才支持事务，MyISAM 引擎不支持事务 存储引擎：存储引擎是MySql中特有的一个术语，其它数据库中没有，比如oracle中有，但是不叫这个名字。 存储引擎听着高端大气上档次，实际上存储引擎是一个表存储/组织数据的方式。不同的存储引擎，表存储数据的方式不同。 事务的基础知识点 只有DM语句（insert、delete、update）才会有事务这一说，其它语句和事务无关! ! !因为只有以上的三个语句是数据库表中数据进行增、删、改的。只要操作一旦涉及到数据的增、删、改，那么就一定要考虑安全问题。 InnoDB存储引擎提供一组用来记录事务性活动的日志文件 MySql默认开启自动提交auto commit，开启MySql的事务start transcation可关闭自动提交，需要手动提交会回滚事务。 提交事务（commit）：清空事务性活动的日志文件，将数据全部彻底持久化到数据库表中。提交事务标志着，事务的结束。并且是一种全部成功的结束。 回滚事务(rollback)：将之前所有的DML操作全部撤销，并且清空事务性活动的日志文件。回滚事务标志着，事务的结束。并且是一种全部失败的结束。 事务的特性ACID： A（Atomicity）原子性：指事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。 C（Consistency）一致性：事务前后数据的完整性必须保持一致。 I（Isolation）隔离性：事务的隔离性是多个用户并发访问数据库时，数据库为每一个用户开启的事务，不能被其他事务的操作数据所干扰，多个并发事务之间要相互隔离。 D（Durability）持久性：持久性是指一个事务一旦被提交，它对数据库中数据的改变应该永久性的，接下来即使数据库发生故障也不应该对其有任何影响。 MySql事务隔离级别4个隔离级别，从低到高 1. 读未提交（ read uncommitted）描述：事务A可读取到事务B的未提交的数据； 存在问题：脏读 这种隔离级别一般都是理论上的，大多数的数据库隔离级别都是2档起步！ 2. 读已提交（ read committed）描述：事务A只能读取到事务B的提交后的数据； 解决了脏读问题； 存在问题：不可重复读 这种隔离级别是比较真实的数据，每一次读到的数据是绝对的真实。oracle数据库默认的隔离级别是: read committed。 3. 可重复读（repeatable read）描述：事务A开启之后，不管是多久，每一次在事务A中读取到的数据都是一致的。即使事务B将数据已经修改，并且提交了，事务A读取到的数据还是没有发生改变，这就是可重复读。 存在问题：幻读 mysql中默认的事务隔离级别是：repeatable read。 4. 串行化（serializable）事务最高隔离级别，效率最低，类似Java中使用synchronize进行线程同步，此处便是事务同步，将事务串行化执行。 事务并发问题脏读事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据。 不可重复读事务 A 多次读取同一条数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果不一致（不能重复读）。 举例，有事务T1、T2 T1：1、根据条件X查询一组数据；2、对该组的每条记录进行修改；3、根据条件X查询一组数据； T2：1、插入满足条件X的一条记录； 当T2的步骤1在T1的步骤1之后执行，提交之后，T1的步骤3再次查询时，发现有一条记录没有被更改，这就是不可重复读。 不可重复读通常针对数据更新（UPDATE）操作来说的。 幻读举例，有两个事务T1、T2 T1：1、查询主键id为1001的数据，结果为不存在；2、插入id为1001的数据； T2：1、插入id为1001的数据； 当T2的步骤1在T1的步骤1之后执行，提交之后，T1的步骤2因为主键冲突将无法执行。就好比T1“见鬼”了，明明查询不存在但是插入失败。 幻读更针对于对数据插入（INSERT）操作和删除（DELETE）来说的。 不可重复读和幻读的区别 网上有很多幻读的例子，但是讲的都略微牵强，经笔者查阅和理解，不可重复的侧重于两次读的操作，即：读-读。而幻读侧重于：读-写。上述幻读的例子中，T1的步骤1不能支持其步骤2的进行。 小结 事务隔离级别 脏读 不可重复读 幻读 读未提交（read uncommitted） 是 是 是 读已提交（read committed） 否 是 是 可重复读（repeatable read） 否 否 是 串行化（serializable） 否 否 否 隔离级别越高，越能保证数据的完整性和一致性，对并发性能的影响也越大。"},{"title":"Java线程池原理（一）","date":"2021-07-10T10:57:11.024Z","url":"/WindShadow/Java%E5%A4%9A%E7%BA%BF%E7%A8%8B/Java%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%8E%9F%E7%90%86/Java%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%8E%9F%E7%90%86%EF%BC%88%E4%B8%80%EF%BC%89/","categories":[["Java多线程","/categories/Java%E5%A4%9A%E7%BA%BF%E7%A8%8B/"],["Java线程池原理","/categories/Java%E5%A4%9A%E7%BA%BF%E7%A8%8B/Java%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%8E%9F%E7%90%86/"]],"content":"Java线程池原理（一）众所周知Executor框架中的ExecutorService接口的实现就是ThreadPoolExecutor线程池，本文《Java线程池原理》系列从源码角度捋一捋线程池的重点实现。 在此之前先聊聊ThreadPoolExecutor的作者：Doug Lea Doug Lea是 java.util.concurrent 包（JUC）的作者，不少网友评价Doug Lea是JDK史上最牛*的程序员，甚至有人感叹：“编程不识Doug Lea，写尽java也枉然”。 接下来盘点ThreadPoolExecutor线程池的基本工作原理和Executors工具类提供的几种常见线程池的效果，简单入个门。 线程池存在的意义我们知道当下常见的jvm使用的线程都是内核级线程（KLT）。显然，线程是稀缺资源，它的创建与销般是一个相对偏重且耗资源的操作，而Java线程依赖于内核线程，创建线程需要进行操作系统状态切换，为避免资源过度消耗需要设法重用线程执行多个任务。 所以线程池就是一个线程缓存，负责对线程进行统一分配、调优与监控。 一般什么时候使用线程池？ 单个任务处理时间比较短 需要处理的任务数量很大 比较耗时的任务不建议使用线程池，因为容易造成任务的堆积 线程池基本工作原理ThreadPoolExecutorjdk提供的线程池实现为ThreadPoolExecutor类（常用就这个类） 其中前3个构造方法都依赖最后一个构造方法，其含义如下 拓扑如下 线程工厂：负责创建线程 工作队列：一个保存任务的阻塞队列（阻塞队列：多线程下保证队列的FIFO特性） Worker：线程池实现线程复用以执行任务的核心对象（见下文），自身实现了Runnable接口且内部维护了一个Thread线程对象 核心线程：Worker对象内的线程，线程池常驻线程，成为核心线程的线程不会被销毁 非核心线程：Worker对象内的线程，成为非核心线程的线程进入空闲后，只会存活一定时间（keepAliveTime），之后便正常结束线程。 拒绝策略：任务无法添加时，线程池执行拒绝策略。ThreadPoolExecutor提供四种如图所示的拒绝策略，实现RejectedExecutionHandler接口自身定义拒绝策略。 线程池的状态类似线程的状态，线程池也有自己的状态 Running：运行状态，接受新任务，也能处理工作队列里的任务 Shutdown：关闭状态，不接受新任务，但是处理工作队列里的任务 Stop：停止状态，不接受新任务，不处理工作队列里的任务，中断正在处理中的任务 Tidying：整理状态，当所有的任务都执行完了，当前线程池已经没有工作线程，这时线程池将会转换为Tidying状态，并且将要调用terminated方法。terminated方法调用完毕之后，线程池进入Terminated状态。 Terminated：终止状态，terminated方法调用完毕之后，线程池进入Terminated状态。 其关系如图（对线程池工作原理有所了解后再回头看此状态转换图会更好）。 线程池添加任务流程我相信大多数人都比较清楚线程池添加任务的流程，各种博客网图巴拉巴拉，这里再哔哔一下线程池提交任务的流程，如下： 我们通常通过执行ThreadPoolExecutor实现Executor接口的execute方法（Executor#execute(Runnable)）来提交任务，这个流程就是来自该方法的实现！！！ 贴一下源码，官方注释给你讲的明明白白，代码也很清晰。 Executors工具类提供的常见线程池可缓存线程池Executors.newCachedThreadPool() 分析：核心线程数为0，最大线程数为Integer.MAX_VALUE，工作队列为同步移交队列； 新增任务：核心线程数为0 =&gt; 任务提交到工作队列 =&gt; 工作队列提交时阻塞直到有线程从队列获取任务。 很显然，每个任务都会交由非核心线程来完成。而非核心线程在任务全部执行完毕后只会存在一定时间（60s），所以这是具有缓存效果的线程池（Cached）。 固定线程数量线程池Executors.newFixedThreadPool() 分析：核心线程数为固定值，最大线程数与核心线程数相同，工作队列为无界队列； 无界队列：队列可存储的元素没有上限，概念上是如此，但实际受数据类型值范围限制，所以无界队列在Java中指可存放元素的最大值为Integer.MAX_VALUE的队列。 LinkedBlockingQueue无参构造方法 新增任务：当核心线程数已满时，任务都存到工作队列，由于工作队列是无界的，所以队列永远不会满（概念上不会满），所以非核心线程没有存在的必要，即maximumPoolSize - corePoolSize = 0。 很显然，在固定线程数量线程池中，提交的每个任务都会交由核心线程来完成，核心线程数量固定，非核心线程数量为0，线程池常驻线程数量固定，所以称之为拥有固定线程数量的线程池（Fixed）。 单线程线程池Executors.newSingleThreadExecutor() FinalizableDelegatedExecutorService，仅重写了finalize方法，调用父类DelegatedExecutorService线程池shutdown方法。 DelegatedExecutorService，该类实现的线程池内部维护了一个实际真正工作的线程池对象，实质是对线程池ExecutorService的一个静态代理。 分析：阅读源码后，不难发现该种线程池最终工作的线程池还是ThreadPoolExecutor的实例，而ThreadPoolExecutor实例的核心线程数为1，最大线程数也为1，工作队列为无界队列； 单线程线程池无非就是数量为1的固定数量线程池（Fixed，corePoolSize = maximumPoolSize = 1），与Fixed线程池不同的是，Single线程池对象在被GC回收时可自动关闭，比较安全，而Fixed线程池的关闭依赖于第三方调用者。 Single线程池更像一个“孤儿”，干活都是同一个Worker，自生自灭。 定时任务线程池 PS：在下博客里的图绝大部分都是自己通过ProcessOn做的，均带有**@WindShadow**水印，这么认真的开发人员哪里找哦 （ ’ - ’ * ) "},{"title":"线程基础与Executor","date":"2021-07-10T04:50:25.252Z","url":"/WindShadow/Java%E5%A4%9A%E7%BA%BF%E7%A8%8B/%E7%BA%BF%E7%A8%8B%E5%9F%BA%E7%A1%80%E4%B8%8EExecutor/","categories":[["Java多线程","/categories/Java%E5%A4%9A%E7%BA%BF%E7%A8%8B/"]],"content":"线程概述线程在现代操作系统中的大致分两种（参考来源：） 内核级线程（KLT，Kernel Level Thread） 线程管理的所有工作（创建和撤销）由操作系统内核完成；操作系统内核提供一个应用程序设计接口API，供开发者使用KLT； 线程交给内核管理，内核需要维护线程表，线程表保存了寄存器、状态和其他信息。当然每个线程所属进程的进程表也是维护在内核的，内核创建和销毁线程的代价是比较大的。 用户级线程（ULT，User Level Thread） 应用程序可以通过使用用户空间运行线程库被设计成多线程程序。线程的创建，消息传递，调度，保存/恢复上下文都有线程库来完成。内核感知不到多线程的存在。内核继续以进程为调度单位，并且给该进程指定一个执行状态（就绪、运行、阻塞等）。 对照一下，就是将线程的调度放在用户态执行，对于内核来说就像是单线程一样。 而Java线程创建是依赖于系统内核，通过JVM调用系统库创建内核线程，内核线程与Java-Thread是1:1的映射关系，当然也有其它映射关系如1：N、N：N，如go语言使用的就是1：N，所以常常go语言被硬吹并发比java高很多就归功于其线程模型。 KLT与ULT（图片来自于网络） Java线程基础创建线程的方式 继承Thread类，重写run方法，新建Thread类对象，使用该对象调用Thread类的start方法启动线程 实现Runnable接口，使用Runnable的实例作为构造方法的参数新建Thread类对象，调用Thread对象的start方法创建线程 实现Callable接口，将Callable实例提交到一个线程池（见下文）ExecutorService中执行任务；准确的说这并不算是创建线程的一个方式，因为ExecutorService接收任务后并不一定创建线程去执行。 Callable接口出现的必然性Callable接口与Runnable接口：Callable的call方法有返回值的，且允许抛出异常，Runnable接口的run方法无返回值，不允许抛出异常（这里的异常肯定是检查型异常）； jdk提供Runnable接口，我们可以在当前线程之外开启子线程去干一些事情（异步），根据Runnable接口的run方法的特点，这个子线程执行了就执行了，我们不清楚这个线程允许情况如何，有时候我们希望子线程执行结束后，我们能拿到些东西，Runnable接口显然并不能满足，于是Callable接口出现了。 显然“任务”这个概念更适合我们对子线程的描述，我们可以这样理解： Runnable是一个不期望有结果的任务，执行了就执行了，自己做好异常处理；Callable是一个期望有结果的任务，我们可以关心它抛出的异常。 下文《Executor框架》将会讲解任务相关的管理。 线程生命周期网上众多讲java线程生命周期时想必都会贴出下面这么一张图，然后就人云亦云 准确的说，上述这5个状态是从cpu角度看操作系统的线程各个阶段的状态。前文说到Java中的线程是KLT，所以Java线程在cpu角度看也是有这5个状态，但是不代表Java就是如此定义线程状态的！！！ JavaThread类使用内部枚举State来标识线程的状态。 可以看到总共有6个状态，下面一一举例 NEW：新生态，线程对象被创建时处于该状态 RUNNABLE：运行态，线程正在运行其线程栈内代码（指令） BLOCKED：阻塞状态，线程想获取不到synchronized同步锁时，进入阻塞状态，如下线程t2获取target对象的锁时，因为线程t1还未释放，所以t2进入阻塞状态 WAITING：等待状态，准确的说是死等，线程调用object.wait()方法，等待其它线程调用object.notify()或object. notifyall()方法，此时该线程进入等待状态。或调用LockSupport.park()方法线程也会进入该状态。 TIMED_WAITING： 等待未超时，线程调用object.wait(long)方法，等待其它线程调用object.notify()或object. notifyall()方法，不过只会等待指定的时间，未超时之前该线程进入等待未超时状态。或线程进入通过sleep方法进行“睡眠”时，也会进入等待未超时状态。或调用LockSupport.parkNanos(Object, long)方法、LockSupport.parkUntil(Object, long)方法线程也会进入该状态。 TERMINATED：终止（死亡）状态，代码执行结束或外部干涉导致线程终止，进入死亡状态。 线程状态的获取通过调用Thread#getState方法获取到当前线程的状态。 线程优先级java线程优先级范围：1~10，优先级从低到高。 优先级越高，越容易得到cpu时间片。 相关方法： 获取线程的优先级：Thread#getPriority 设置线程优先级：Thread#setPriority 守护线程 java中线程分为用户线程（User Thread ）和守护线程（Daemon Thread ） 虚拟机必须确保用户线程执行完毕 虚拟机可以不等待守护线程执行完毕 通过调用Thread#setDaemon(true)方法将线程设置为守护线程 守护线程最典型的应用就是 GC (垃圾回收器)。用户线程和守护线程两者几乎没有区别，唯一的不同之处就在于，当用户线程全部执行结束，只剩下程存在了，虚拟机也就退出了， 因为没有了被守护者，守护线程也就没有工作可做了，也就没有继续运行程序的必要了。 停止线程的方法 Thread#stop方法，过时，不建议使用 Thread#destroy方法，废弃 设置某种标志位，结束线程代码的执行，正常停止线程 线程休眠调用Thread.sleep方法使当前线程进入阻塞状态，线程结束休眠后进入就绪态； 线程通过Thread.sleep方法进入休眠时，不会释放对象锁 线程礼让调用Thread.yield()方法使当前线程让出cpu时间片，使线程从运行态重新进入就绪态。值得注意的是：A线程让出cpu时间片，不代表B线程就可以获取到cpu时间片，下一次获取cpu时间片的线程可能还是A。 线程“插队”如在线程A的代码中，调用B线程对象的Thread#join方法，将B线程加入到A线程，A线程需要等待B线程执行完之后才能继续执行自己的代码。好比插队，可传入等待时间表示允许插队的最长时间，超过该时间，B线程还未执行结束，则不再等待。 Executor框架下面一一引出Executor框架相关的类或接口 Executor前文我们将Runnable 和Callable称之为任务，那么任务有了，就要有对应的任务执行者和管理者，如果不进行管理，直接使用new Thread()的方法创建线程有很多缺点： new Thread()耗费性能 调用new Thread()创建的线程缺乏管理，被称为野线程，而且可以无限制创建，之间相互竞争，会导致过多占用系统资源 不利于扩展，比如如定时执行、定期执行、线程中断等 所以任务（线程）的管理是必须的。 对于Runnable，我们不期望有结果（无返回值），只管执行，所以Executor接口出现了。 通过Executor的executor方法来启动任务（子线程），更加便捷，并且可以避免“this逃逸”问题 this逃逸问题this逃逸是指在构造函数返回之前其他线程就持有该对象的引用，调用尚未构造完全的对象的方法可能引发奇怪的错误。 代码示例： 可修改如下规避this逃逸 ExecutorService对于Callable，也需要有一个执行者和管理者，于是ExecutorService出现了，无论是Callable还是Runnable，这些任务都涉及管理和扩展方面的优化，ExecutorService索性直接扩展Executor增加对Callable支持。 ExecutorService提供了任务生命周期管理等功能 ，实际开发中用的更多的是ExecutorService，它的底层实现就是线程池。 Future任务提交后，我们肯定需要知道任务执行得怎么样了，而ExecutorService使用Future来跟踪任务，通过ExecutorService提交任务后就可以得到Future对象。 Future接口的主要方法如下： boolean cancel(boolean mayInterruptIfRunning); 尝试取消任务，成功则返回true，失败false（任务已经完成、已经被取消或由于其他原因无法取消） mayInterruptIfRunning 参数：是否中断任务，true，中断任务且取消任务，false，允许任务继续执行到结束，但是获取结果时会抛异常。 boolean isCancelled(); 任务是否已经取消 boolean isDone(); 任务是否执行结束 V get() throws InterruptedException, ExecutionException; 获取任务执行结果，获取不到结果之前会一直等待，如果任务已经取消了则抛出异常 V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException; 获取任务执行结果，但只会等待指定的时间，超时则抛出异常。 timeout 参数：时间数量 unit 参数：时间单位 ExecutorsExecutors类是Executor体系相关的工具类，通过它可以很方便的操作Executor体系相关的接口示例，如创建线程池。 总结以上就是Executor框架的主要知识点，总结一下，Executor框架要由三大部分组成： 任务(Runnable /Callable) 任务的执行(Executor) 异步计算的结果(Future) Executor 框架的使用 主线程首先要创建实现Runnable或者Callable接口的任务对象，把任务对象交给ExecutorService执行，得到结果对象Future，最后，主线程可以通过对象Future来获取任务结果或取消任务。 本文参考B站UP主“狂神说Java”的Java多线程教学视频进行整理。 感谢成长路上为在下传道受业解惑之人"},{"title":"MyBatis源码一 MyBatis执行器与缓存实现","date":"2021-06-25T12:11:10.029Z","url":"/WindShadow/MyBatis/MyBatis%E6%BA%90%E7%A0%81/MyBatis%E6%89%A7%E8%A1%8C%E5%99%A8%E4%B8%8E%E7%BC%93%E5%AD%98%E5%AE%9E%E7%8E%B0/","categories":[["MyBatis","/categories/MyBatis/"],["MyBatis源码","/categories/MyBatis/MyBatis%E6%BA%90%E7%A0%81/"]],"content":"开门见山，上菜：MyBatis执行器脑图，阅读过程中反复食用即可理解（ ’ - ’ * ) SqlSessionSqlSession接口上的注释 翻译一下：使用MyBatis的主要Java接口。通过这个接口，您可以执行命令、获取映射器和管理事务 SqlSession接口定义了如：&lt;T&gt; T selectOne(String statement);、&lt;E&gt; List&lt;E&gt; selectList(String statement);、&lt;K, V&gt; Map&lt;K, V&gt; selectMap(String statement, String mapKey);、int insert(String statement);、int update(String statement);、int delete(String statement, Object parameter);、void commit();等的一系列增删改查和事务提交/回滚接口，方便开发者调用，MyBatis提供的SqlSession接口实现之一是DefaultSqlSession，实际工作干活的就是它。 根据MyBatis执行器脑图可知其内部又是通过Executor执行器去干活的 SqlSession的增删改查无论是怎么查，怎么改，怎么删、怎么加，最终都是调用Executor的“update改”和“query查”两个接口（见下文），这就是SqlSession使用的设计模式：门面模式，对外提供友好的api方法，内部屏蔽了调用Executor方法的复杂性。 ExecutorExecutor（org.apache.ibatis.executor.Executor）是一个接口，称之为sql执行器。 其定义update(增、改、删)、query(查)、commit(提交事务)、rollback(回滚事务)等操作。 几个重要的方法： 增、改、删 参数含义如下 MappedStatement ms：SQL映射语句（Mapper.xml文件每一个方法对应一个MappedStatement对象） Object parameter：参数，通常是List 查询方法 参数含义（不赘述出现过的参数类型）如下 RowBounds：行边界，主要保存分页参数（limit、offset） ResultHandler resultHandler：结果处理器，入参时一般为null，实际的结果处理器由Configuration配置对象和MappedStatement对象生成 可提供缓存key的查询方法 参数含义（不赘述出现过的参数类型）如下： CacheKey：缓存的key对象 BoundSql boundSql：可以通过该对象获取SQL语句，MyBatis保存sql语句的对象。 创建缓存Key（MyBatis一二级缓存的缓存Key） 可以看出缓存Key由上述参数（SQL映射语句MappedStatement、参数Object、行边界RowBounds、sql语句对象BoundSql）来决定。 下面一一介绍Executor的实现类：BaseExecutor、SimpleExecutor、ReuseExecutor、BatchExecutor、CachingExecutor，其实还有一个ClosedExecutor，代表已经关闭的Executor，是ResultLoaderMap的私有内部类，此处不展开阐述。 BaseExecutorExecutor的抽象实现，实现执行器的公共操作：一级缓存、连接获取等，查询、更新具体的实现由其子类来实现 具体的查询操作：doQuery方法 具体的更新操作：doUpdate方法（包括增删改） SimpleExecutor简单执行器，继承BaseExecutor，无论执行的sql如何，每次都会生成预编译java.sql.PreparedStatement对象。 ReuseExecutor可重用执行器，继承BaseExecutor，相同的sql（肯定是带占位符的）只进行一次预编译（缓存），即预编译对象可重用。 缓存大致原理： 内部维护一个map（key: sql, value: Statement对象）作为预编译对象的缓存 在执行doUpdate或doQuery方法时先查缓存，为命中则生成新的预编译对象且加入缓存map中（如图） BatchExecutor批处理执行器，继承BaseExecutor，该执行器专为批处理场景设计； （解释一下批处理场景： 假设我们需要遍历一个用户对象集合，对每个用户年龄进行加1操作然后更新，假设我们使用ReuseExecutor，那么每个用户的更新操作都会向数据库发送一次sql，而批处理操作便是一次性向数据库发送多条sql。） BatchExecutor属性成员 值得注意的是： 对查询操作，同简单执行器SimpleExecutor一样，批处理执行器每次都会生成预编译对象 对于更新操作(增删改)，BatchExecutor对象本身会记录当前的sql和MappedStatement，如果下一次更新操作的sql和MappedStatement与维护的sql和MappedStatement都相同，则直接复用，否则替换掉当前维护的sql和MappedStatement BatchExecutor需要调用flushStatements()方法刷新statement，数据库内的数据修改才会生效。 执行BatchExecutor的doQuery方法时，会先执行flushStatements()方法，再进行查询操作 下面贴出BatchExecutor实现doUpdate方法的源码，并加以注释。 通过阅读BatchExecutor的成员变量和doUpdate方法的源码，不难发现，以下几点： 通过statement列表保存要执行的sql操作 BatchExecutor内部通过维护一个sql和一个MappedStatement来减少statement的生成，连续相同的sql和MappedStatement不会生成新的statement 通过批处理结果对象（BatchResult）列表，维护批处理结果，其中批处理结果对象维护了MappedStatement、sql以及不同参数的列表。 statement列表和批处理结果对象（BatchResult）列表，它们下标对应的元素是一一对应有关联的 CachingExecutor缓存执行器，实现Executor接口，实现二级缓存，详情见下文。 一缓存实现原理首先要明确一级缓存存在的意义（需求）：同一个事务内，多次相同的查询每次都查询数据库性能是不高的，所以要做缓存 所以sql执行器Executor接口干净利落的定义了一级缓存（也叫localCache）相关的接口，因为一级缓存是一个逻辑上必然应该存在的功能。 前文讲到MyBatis的一级缓存是由BaseExecutor实现的，BaseExecutor通过内部的PerpetualCache localCache缓存对象来维护缓存。 PerpetualCache 内部通过维护一个map实现MyBatis的Cache接口（此处不展开MyBatis的缓存接口的实现体系）。 一级缓存源码分析我们将BaseExecutor实现Executor接口的两个query方法源码贴出并加以关键注释，即可明白BaseExecutor如何实现MyBatis一缓存的，如下： 不带缓存key的查询方法 带缓存key的查询方法（重点） 查询数据库的方法queryFromDatabase 一级缓存清除时机 BaseExecutor更新操作（方法）被调用时，清除执行器缓存的数据 根据一级缓存的需求，事务提交后，缓存应该就要被清空，这一点在BaseExecutor的 commit方法中体现 BaseExecutor实现一级缓存总结 如何维护：通过PerpetualCache localCache成员变量维护一级缓存 写入时机：实现Executor接口的query方法时，先尝试从缓存中获取查询结果，获取不到则调用子类的doQuery方法从数据库获取查询结果，获取到数据后写入缓存再返回 清除时机：【1】执行BaseExecutor实现Executor执行器接口的update更新方法时，先清除一级缓存的所有数据，再调用子类的doUpdate方法执行更新操作；【2】BaseExecutor的commit方法被调用时清除一级缓存的所有数据 二级缓存实现原理同样，先明确二级缓存存在的意义（需求）：我们希望数据的缓存结果可以覆盖整个应用，也就是多个事务，一个事务的查询结果被缓存后，另一个事务也能读取到这个缓存结果，以减少对数据库的访问次数，加快数据查询效率。 MyBatis的二级缓存是通过CachingExecutor实现的，需要注意的是，CachingExecutor在Executor类结构中和BaseExecutor是同级的，内部维护了一个Executor对象，通过构造方法传入 CachingExecutor专门负责二级缓存，而获取连接等执行器的基本操作则交给给内部的Executor对象，此处用到了设计模式中的“装饰者模式（装饰器模式）”，在不改变原有功能的基础上，增加新功能，这里的新功能就是二级缓存的功能了。 （有的文章说这是委派模式，CachingExecutor的Executor delegate变量名称确实是委派的意思。不过根据笔者的理解和查询其它高质量的文章之后，比较赞同该设计模式为装饰者模式，因为CachingExecutor确实做到了“在不改变原有功能的基础上，增加新功能”，并且委派模式中，通常“委派者”在委派任务时，通常要根据条件决定把任务交给哪个具体的实现对象，CachingExecutor显然不是这样的情形） 二级缓存源码分析CachingExecutor实现二级缓存的操作类似于一级缓存，在实现Executor接口的query方法时先查缓存，查不到则调用内部的Executor对象的query方法获取。下面分析CachingExecutor的query方法。 不带缓存key的查询方法，最终调用带缓存key的查询方法，与一级缓存相同 带缓存key的查询方法（重点） 二级缓存清除时机 CachingExecutor更新操作（方法）被调用时，清除所有缓存的数据 flushCacheIfRequired刷新（清除）缓存，如果可以需要的话 二级缓存跨事务使用具体实现与数据一致性原理（重点）思考三个点： 由二级缓存的需求可知，二级缓存服务于多个事务的，因为同一个事务内的多次查询已经由一级缓存来保证效率了，所以显然缓存数据的写入应该在事务提交时才会写入。 而CachingExecutor提交方法commit是直接调用事务缓存管理器对TransactionalCacheManager的提交方法，毫无疑问二级缓存数据的写入操作肯定与该方法有关 又通过CachingExecutor的query方法源码分析可知，二级缓存的数据的存储实际由MappedStatement对象的自身的cache缓存来做的，而缓存的添加和删除是调用TransactionalCacheManager的方法实现的 而二级缓存在得到查询结果后就直接通过事务缓存管理器对象TransactionalCacheManager缓存结果了，但是此时事务并没有提交。 由以上三点可知，CachingExecutor通过MappedStatement的cache缓存和事务缓存管理器TransactionalCacheManager的配合来实现二级缓存 来看看事务缓存管理器TransactionalCacheManager管理啥了 不难看出TransactionalCacheManager实际管理的是MappedStatement的cache缓存和事务缓存对象TransactionalCache的关系，TransactionalCacheManager的方法最终执行的都是TransactionalCache的方法。 带着这个问题阅读TransactionalCache的源码（读者自行阅读），理解TransactionalCacheManager、cache缓存、TransactionalCache的关系就全懂了。总结出下图 TransactionalCache源码总结 TransactionalCache的四个成员变量含义如上图所示 二级缓存数据添加流程： CachingExecutor的query查询操作通过事务管理器（tcm）添加缓存时，tcm根据该缓存对象获取对应的事务缓存对象TransactionalCache（tc） tc将要添加的二级缓存的key-value对先添加到一个由hashmap实现的暂存区 在执行commit方法（CachingExecutor -&gt; TransactionalCacheManager -&gt; TransactionalCache）时，tc将暂存区的数据添加到缓存cache（MappedStatement的cache）中，同时将记录过未命中缓存的key也加入到缓存cache中，这些key的value为null。 二级缓存未命中时，tc记录当前未命中的key到一个由hashset实现的缓冲区中 clearOnCommit的作用： TransactionalCache通过clearOnCommit变量标记，通知事务进行提交时，决定是否先清空二级缓存（MappedStatement的cache），后以当前事务的发生的缓存为最新缓存，刷新到二级缓存中。 clearOnCommit = true时，则认为当前二级缓存中的数据无效，调用TransactionalCache的getObject方法获取缓存中的数据必定是null TransactionalCache每次提交或回滚后，调用内部的reset方法将clearOnCommit变量设置为false（见上图commit方法），以便后续的事务可以读到二级缓存的数据。 而clearOnCommit = true 的情况只有在调用TransactionalCacheManager 的clear方法时（TransactionalCacheManager -&gt; TransactionalCache）才会出现 而只有在调用CachingExecutor的flushCacheIfRequired方法时可能调用TransactionalCacheManager 的clear方法（见前文源码），追溯到底，若想触发clearOnCommit = true，则需要触发MappedStatement#isFlushCacheRequired为true，当执行更新操作时上述情况才会发生。 得出二级缓存数据一致性原理：执行器执行更新操作时，MappedStatement#isFlushCacheRequired为true，对应缓存的TransactionalCache的clearOnCommit = true，达到在没有事务提交或回滚前，缓存查询不命中的效果，保证数据的一致性。 二级缓存命中示例调用执行器的commit方法进行提交时，二级缓存中才会有数据，后续的查询操作才能命中缓存，示例代码如下 因为二级缓存是跨线程跨连接调用的，所以需要设计成提交后数据才会写入缓存，而一级缓存是单线程内调用，所以无需提交。 CachingExecutor实现二级缓存总结 CachingExecutor使用“装饰者模式”，装饰一级缓存的Executor执行器对象，增加二级缓存的功能 查询操作时，查询顺序是：二级缓存 -&gt; 一级缓存 -&gt; 数据库 CachingExecutor本身不维护二级缓存，而是作为MappedStatement的缓存的调用者，所以二级缓存也被称为“Mapper级别的缓存”。事务缓存管理器对象TransactionalCacheManager 、TransactionalCache、MappedStatement三者的搭配实现了二级缓存底层功能。 执行CachingExecutor实现Executor执行器接口的update更新方法时，会先清除二级缓存的所有数据，再调用被装饰执行器的update方法执行更新操作 二级缓存需要手动开启，mapper.xml文件中需要指定&lt;cache/&gt;标签以开启二级缓存，这样生成的MappedStatement才会有Cache缓存对象，MappedStatement#getCache()方法返回值不是null（见query方法源码），CachingExecutor才能使用到MappedStatement的缓存。 一些思考 二级缓存线程安全吗，一级缓存呢？ 二级缓存线程不安全，二级缓存是mapper级别的缓存，是跨线程跨连接的，实际的实现是MappedStatement来实现的，底层存储也是HashMap而不是ConcurrentHashMap，二级缓存也不用也没必要实现线程安全，因为SqlSession本身就是线程不安全的。 一级缓存肯定是线程安全的，因为一级缓存的读写只可能在一个线程，一个连接里发生。（如果你强行将BaseExeCutor让多个线程调用，那当我没说） 二级缓存能保证100%数据一致性吗？ 不能，因为MappedStatement对象是SQL映射语句的封装，那么当这个sql操作过的表，在其他mapper的sql里也操作了，对应MappedStatement的缓存肯定是不感知的，所以二级缓存的数据一致性，需要开发者自己注意，MyBatis的CachingExecutor在代码层面上有效保证了数据一致性，但是实际的sql语句造成的影响需要开发者控制，这也是为什么二级缓存需要手动开启的原因。 二级缓存为什么不在BaseExecutor实现？ 二级缓存如果在BaseExecutor实现，那么BaseExecutor必然需要怎么某种开关去控制二级缓存的开启与关闭，这就造成BaseExecutor的职责不单一，面向对象编程应该要让一个类职责明确。 二级缓存为什么不像一级缓存一样通过类继承实现？ 如此做会造成Executor类体系结构过于复杂，使用装饰者模式在不改变原有功能的基础上增加二级缓存功能是很好的选择。 三级缓存甚至n级缓存如何实现？ 参考二级缓存的实现，继续使用装饰者模式实现三级缓存甚至n级缓存都是ok的。 推荐阅读MyBatis源码阅读指南【鲁班大叔】： 本文参考B站UP主“鲁班大叔”的MyBatis源码分析教学视频进行整理； 感谢成长路上为在下传道受业解惑之人"},{"title":"Nginx调优","date":"2021-06-23T16:41:11.610Z","url":"/WindShadow/Nginx/nginx%E8%B0%83%E4%BC%98/","categories":[["Nginx","/categories/Nginx/"]],"content":"Nginx调优并发数基于nginx的多进程架构，通过合理配置worker_processes、worker_connections、worker_rlimit_nofile参数可调节nginx的最优并发。 worker_processes在nginx配置文件全局块中，通过 worker_processes 参数配置worker 进程数，设置为 auto时，nginx则会自动设置与cpu核心数相同的数量worker 进程。 worker_connection 这个值是表示每个worker进程所能建立连接的最大值，所以，一个nginx能建立的最大连接数，应该是 worker_connections * worker_processes。当然，这里说的是最大连接数，对于HTTP请求本地资源来说，能够支持的最大并发数量是worker_connections * worker_processes，如果是支持http1.1的浏览器每次访问要占两个连接（ 并不是request和response响应占用两个连接 ），所以普通的静态访问最大并发数是：worker_connections * worker_processes / 2，而如果是HTTP作为反向代理来说，最大并发数量应该是worker_connections * worker_processes / 4，因为作为反向代理服务器,每个并发会建立与客户端的连接和与后端服务的连接，即2*2=4。 worker_rlimit_nofile这个参数表示worker进程最多能打开的文件句柄数，基于liunx系统ulimit设置（Linux一切皆文件，所有请求过来最终目的访问文件，查看系统文件句柄数最大值（用户可以打开文件的最大数目）：ulimit -n；一般root用户是65535，普通用户是1024） worker_rlimit_nofile 理论值应该是最多打开文件数（ulimit -n）与nginx worker进程进程数相除，但是nginx分配请求并不是那么均匀，所以与ulimit -n的值保持一致为优。 所以 worker_connections 值不能超过 worker_rlimit_nofile ，否则在高并发下可能会出现“too many open files”的异常。 nginx长连接保持和客户端的长连接一般情况下，nginx已经自动开启了对客户端（浏览器）连接的keep alive（http1.1）支持。 同时可设置以下两个配置： keepalive_timeout指令格式：keepalive_timeout timeout [header_timeout]; timeout ：设置keep-alive客户端连接在服务器端保持开启的超时值（默认75s）；值为0即禁用keep-alive客户端连接；header_timeout：可选，在响应的header域中设置一个值“Keep-Alive: timeout=time”；通常可不设置；timeout默认75s，一般情况下也够用，对于一些请求比较大的内部服务器通讯的场景，适当加大。 keepalive_requestskeepalive_requests指令用于设置一个keep-alive连接上可以服务的请求的最大数量，当最大请求数量达到时，连接被关闭，默认为100。 该参数的工作机制：一个keep alive建立之后，nginx就会为这个连接设置一个计数器，记录这个keep alive的长连接上已经接收并处理的客户端请求的数量。如果达到这个参数设置的最大值时，则nginx会强行关闭这个长连接，逼迫客户端不得不重新建立新的长连接。 保持和server的长连接nginx访问后端server（nginx称为upstream，上游服务器）默认用的是短连接的HTTP1.0协议，客户端请求到达时，nginx与后端建立连接，后端响应完毕后主动关闭该连接。 为了让nginx和后端server之间保持长连接，一般指明nginx请求后端服务的协议为http1.1 其中proxy_set_header Connection &quot;&quot;;指令作用时清理来自客户端的Connection请求头，因为http1.1保持开启的长连接的关键是Connection请求头， 因为如果客户端和nginx之间是短连接（http1.0），我们把Connection请求头也带到后端，那么nginx与后端就不会开启长连接了，所以此处需要清除。 upstream块的keepalive官方解释：设置到upstream服务器的空闲keepalive连接的最大数量， 当这个数量被突破时，最近使用最少的连接将被关闭，keepalive指令不会限制一个nginx worker进程到upstream服务器连接的总数量。 说白了就是nginx往后端服务器发的请求的空闲连接的最大数量，假设keepalive设置为30，此时来了100个请求，nginx新建了100个线程去请求后端，那么请求结束后，nginx会关闭100-30=70个线程，此时又再来了100个请求，nginx必须再新建70个线程去请求后端，之前30个线程被复用。所以合理配置keepalive是很有必要的。 一般把upstream块的keepalive配置和保持和server长连接的配置结合使用。 持续更新……"},{"title":"Docker容器数据卷","date":"2021-05-26T12:01:46.804Z","url":"/WindShadow/Docker/docker%E5%AE%B9%E5%99%A8%E6%95%B0%E6%8D%AE%E5%8D%B7/","categories":[["Docker","/categories/Docker/"]],"content":"我们知道docker容器相当于一个小型linux系统，docker容器被删除后，这个系统也就消失了，如果数据都在容器中，那么删除容器后数据就会丢失，如mysql容器中数据库表数据，docker提供了数据卷来对容器内的数据进行持久化。 数据卷挂载 作用：将容器内的文件映射（挂载）到宿主机，类似硬链接。 数据卷是一个可供容器使用的特殊目录，它绕过文件系统，可以提供很多有用的特性： 数据卷可以在容器之间共享和重用。 对数据卷的更改会立即生效。 对数据卷的更新不会影响镜像。 和容器的生命周期是分离的，数据卷会一直存在，除非被清理。 数据卷的变化不会影响镜像的更新。数据卷是独立于联合文件系统，镜像是基于联合文件系统。镜像与数据卷之间不会相互影响。 上图表示了docker挂载卷的3种方式：volume、bind mount、tmpfs mounts； volume挂载方式volume意思为“卷”，可在执行docker run命令时使用-v选项挂载卷，docker管理数据卷的目录一般为/var/lib/docker/volumes/ 匿名挂载命令示例：docker run -v &lt;容器内路径&gt; centos；此方式为匿名挂载，docker将分配一个匿名的数据卷（如上图16进制字符构成的文件夹），将容器内的路径硬链接到该卷下的_data目录。 具名挂载命令示例：docker run -v &lt;卷名称&gt;:&lt;容器内路径&gt; centos；此方式为具名挂载，docker将分配一个有具体名称的数据卷（如上图mycentos-v文件夹），如果该名称卷已经存在则直接使用而不再创建，将容器内的路径硬链接到该卷下的_data目录。 数据卷挂载文件覆盖问题： 如果容器内路径不存在则被创建 如果挂载一个空的数据卷到容器中的一个非空目录中，那么这个目录下的文件会被复制到数据卷中。 如果挂载一个非空的数据卷到容器中的一个目录中，那么容器中的目录中会显示数据卷中的数据。如果原来容器中的目录中有数据，那么这些原始数据会被隐藏掉。 这两个规则都非常重要，灵活利用第一个规则可以帮助我们初始化数据卷中的内容。掌握第二个规则可以保证挂载数据卷后的数据总是你期望的结果。 除此之外，也可以在通过Dockerfile构建镜像时指定挂载出匿名数据卷。 bind mounts绑定挂载绑定文件系统的文件。 命令示例：docker run -v &lt;宿主机路径&gt;:&lt;容器内路径&gt; centos ；此方式为指定路径挂载，此挂载方式，docker直接将容器内的路径硬链接到指定的宿主机目录，有以下几点值得注意： 且宿主机路径必须是绝对路径 无论宿主机路径还是容器内路径，路径不存在则被创建 文件覆盖规则：【宿主机 -&gt; 容器内】 此种挂载方式无法通过docker volume管理卷。 只读挂载过ro选项指定挂载的数据卷是只读的，意味着容器无法对挂载的文件进行修改，只能读，即挂载的文件只允许宿主机单向修改。rw便是可读可写，默认卷读写性质为rw。 匿名挂载的只读：匿名挂载不支持只读，即使使用--read-only选项进行只读限定，也不会生效，卷读写性质依旧为rw 具名挂载的只读：命令示例：docker run -v &lt;卷名称&gt;:&lt;容器内路径&gt;:ro centos bind mounts挂载的只读：命令示例：docker run -v &lt;宿主机路径&gt;:&lt;容器内路径&gt;:ro centos；文件覆盖情况【宿主机 -&gt; 容器内】且是强制覆盖，且容器无法对该目录的文件进行修改 tmpfs临时文件系统volume挂载方式（卷挂载）和bind mounts挂载方式（绑定挂载）允许在主机和容器之间共享文件，以便即使在容器停止后也可以保留数据。Linux上运行的Docker，有第三个挂载方式选择：tmpfs，即临时的文件系统。 以tmpfs挂载方式创建容器时，容器可以在容器的可写层外创建文件。与卷和绑定挂载相反，tmpfs挂载是临时的，并且仅保留在主机内存中。当容器停止时，将tmpfs删除。这对于临时存储不希望在主机或容器可写层中保留的敏感文件很有用。 tmpfs挂载的限制： 与卷和绑定挂载不同，tmpfs无法在容器之间共享 只有在Linux上运行的Docker才能使用此功能 命令实例：docker run --rm -it --tmpfs /root centos； tmpfs挂载允许两个配置选项，都不是必需的。如果需要指定这些选项，则必须使用该--mount选项，因为该–tmpfs选项不支持它们： tmpfs-size：tmpfs挂载的大小（单位：字节），默认无限 tmpfs-mode：tmpfs的文件模式，为八进制，即linux系统下的文件读写权限的数字标识，例如700或0770，默认值为1777（全局可写） 命令实例：docker run --rm -it --mount type=tmpfs,destination=/root,tmpfs-mode=1770 centos 关于--mount与--tmpfs的区别： 数据卷管理docker volume作用：可以通过docker volume命令管理数据卷 常见命令格式：docker volume &lt;option&gt; docker volume ls：查看所有数据卷 docker volume rm &lt;卷名称&gt;：删除数据卷，还有容器使用的数据卷不可被删除 docker volume inspect &lt;卷名称&gt;：查看数据卷详情 docker volume create &lt;卷名称&gt;：创建一个数据卷 数据卷容器容器挂载数据卷，其它容器通过挂载这个（父容器）实现数据共享，这个挂载数据卷的“父”容器称之为数据卷容器； 因为数据卷会存在直到没有容器使用为止（除非手动强制删除），像多个数据库容器场景下，容器之间需要传递共享数据卷，使用容器维护数据卷，这就是数据卷容器。 –volumes-from可在执行docker run命令时使用--volumes-from选项从其它容器挂载卷。 命令示例：docker run -it --volumes-from=&lt;容器id或容器名&gt; centos，数据覆盖情况：【卷 -&gt; 容器内】； 我们可以把--volumes-from选项比作继承，上图中，容器X挂载出volume1、volume2，数据卷容器A和容器B继承自容器X，容器C继承容器A，容器D继承容器B，最终ABCD容器都挂载了volume1、volume2，无论哪个容器对数据卷内的文件进行修改，其它容器都能同步更新到，容器X便是数据卷容器。"},{"title":"Docker网络","date":"2021-05-26T12:01:46.401Z","url":"/WindShadow/Docker/docker%E7%BD%91%E7%BB%9C/","categories":[["Docker","/categories/Docker/"]],"content":"docker0网卡服务器安装了docker，就会有一个网卡docker0，docker0网卡使用nat直连到服务器的物理网卡。 evth-pairevth-pari 是一种虚拟网络设备，是一对的虚拟设备接口，它们都是成对出现的，一端连着内核协议栈 ，一端彼此相连着，一个设备收到协议栈的数据发送请求后，会将数据发送到另一个设备上去，正因为这个特性，evth-pair 充当一个桥梁，连接各种虚拟网络设备；OpenStack，Docker容器之间的连接, OVS的连接，都是使用 evth-pair 技术。 上图中，docker0扮演的角色可以比作容器依赖的路由器，容器不指定网络的情况下，都是通过docker0路由的，docker会给容器分配一个默认的可用ip，只要容器被删除，对应的evth-pari接口就没了 。docker中通过evth-pair连接到docker0的网络连接模式称之为桥接模式，该模式下的任意容器之间是可以相互ping通的。 桥接到docker0网卡的方式缺点也很明显，无法预知容器IP，且在容器重启后IP可能会发生变化，导致容器内的应用之间存在依赖时，不方便配置，如数据库IP等。 docker0 IP分配方式需要注意的是，docker0分配IP的方式是按顺序分配，如已经分配了容器A（172.17.0.1）、容器B（172.17.0.2），下一个容器C的IP为172.17.0.3，如果此时容器B停止或被删除，再新建容器D时，其IP为172.17.0.2。 link方式连接容器桥接到docker0的网络连接方式显然不能满足部署需求，如果能使用容器名进行网络连接，就不必担心容器重启后IP改变的问题了，docker提供了link方式的网络连接； 在docker run启动容器时使用--link选项连接到其它容器：命令示例：docker run -it --link &lt;容器id或容器名&gt;:[alias] centos； docker run -it --name=mycentos --link mysql-dev centos：启动一个名为mycentos的容器，且连接到名为mysql-dev的容器，此时在mycentos容器内可以使用ping mysql-dev的方式ping通名为mysql-dev的容器 docker run -it --name=mycentos --link mysql-dev:db centos：alias是容器在link模式下的别名，同上，在mycentos容器内还可以使用ping db的方式ping通名为mysql-dev的容器 link方式连接原理在源容器的hosts文件写入目标容器名、目标容器link下的别名、目标容器id，均指向目标容器的IP，当目标容器的IP发生变化时，hosts文件的配置也跟着发生变化，这是通过docker容器的环境变量完成的。 缺点 显然的，源容器通过link方式连接到目标容器时，可以通过容器名访问目标容器，反过来则不行，因为是通过写hosts的形式去实现的。 当目标容器被删除时，源容器的hosts配置还存在，此时新的容器“占据”旧目标容器IP时（依据docker0的 IP分配方式），源容器的hosts配置并不会发生改变，故而连接到新的容器。 官方已经已经不推荐使用link方式去设计容器的网络了。 自定义网络（核心）既然docker通过容器模拟了一个小型的linux操作系统，自然也少不了计算机网络。docker提供docker network命令来管理网络，使用docker创建网络的主要参数有：网络名称，网络模式，子网，网关。 docker的网络模式 bridge 桥接模式，docker默认的网络模式，该模式下容器桥接到docker0网卡 none 不配置网络 host 和宿主机共享网络，即使用宿主机的IP端口 container 容器网络连通（用得少，局限大） 网络管理docker network create作用：创建网络； 命令格式：docker network create [options] &lt;网络名&gt;； 常见命令格式：docker network create --driver &lt;网络模式&gt; --subnet &lt;子网&gt; --gateway &lt;网关地址&gt; &lt;网络名称&gt; ； 常用option选项： --driver：指定网络模式，不指定默认为桥接模式(driver)，一般使用桥接模式较多 --subnet：指定子网，写法为网段 + 掩码，如10.0.0.0/16 --gateway：指定网关地址 --ip-range：指定ip网络内可动态分配的IP范围，写法为网段 + 掩码，如10.0.0.0/16 示例：docker network create --driver bridge --subnet 10.0.0.0/16 --gateway 10.0.0.254 mynet，该命令创建了一个网络，名称为”mynet”，网络模式为桥接模式（bridge ），子网为10.0.0.0/16，网关为10.0.0.254，此时宿主机网络情况如下 docker network ls查看当前所有网络；常见命令格式：docker network ls [option]； 示例：docker network ls -f &#39;driver=bridge&#39;：查看所有网络中网络模式为桥接的网络。 docker network inspect查看网络详情；常见命令格式：docker network inspect &lt;网络id或网络名称&gt;； 该命令可查看网络创建时的基本信息和使用该网络的容器信息等。 docker network rm删除网络；常见命令格式：docker network rm &lt;网络id或网络名称&gt;； 容器连接或断开网络可以使用docker run新建容器时可使用--network或--net将容器连接到指定的网络：docker run -it --net &lt;网络id或网络名&gt; centos，另一种便是使用docker network connect命令。 docker network connect作用：将容器连接到网络； 常见的命令格式：docker network connect &lt;网络id或网络名称&gt; [options] &lt;容器名&gt;； option选项： --alias ：指定该容器在该网络中的别名 --ip：指定IP地址（常用） --ip6：指定IPv6地址 --link：链接到另一个容器 --link-local-ip：添加容器的链接本地地址 docker network connect的选项几乎全都可在docker run时使用。 docker network disconnect作用：将容器与网络断开； 常见的命令格式：docker network disconnect [option] &lt;网络id或网络名称&gt; &lt;容器名&gt;； 断开条件：容器必须正在运行才能将其与网络断开连接； option选项： -f：强制断开容器与网络的连接 容器连接到网络之后，容器内都有一个默认的DNS服务器配置，这样任意容器之间均可通过容器名互相访问，而不需要像link方式般由hsots文件维护容器名与IP的关系。 停止、暂停或重启容器对网络的影响暂停、重新启动和停止连接到网络的容器，容器在运行时会连接到其配置的网络： 若未容器指定IP（动态IP），容器启动时自动获取网络内可分配的IP 若未容器指定了IP（静态IP），容器启动时应用容器的 IP 地址，如果 IP 地址不再可用，则容器无法启动。 保证 IP 地址可用的一种方法： 使用--ip-range指定一个网络自动分配的IP的范围，使用--ip给容器指定该范围之外的IP地址（静态IP），可确保在此容器不在网络上时不会将 IP 地址提供给另一个容器。 如：docker network create --driver bridge --subnet 10.0.0.0/24 --ip-range 10.0.0.0/25 --gateway 10.0.0.254 mynet，创建一个网络名叫”mynet”，子网为10.0.0.0/24，可动态分配的地址范围为10.0.0.0/25，网关为10.0.0.254，新建容器A、容器B、容器C且都加入mynet网络，容器C使用--ip指定IP地址。 "},{"title":"Docker基础","date":"2021-05-21T12:31:15.199Z","url":"/WindShadow/Docker/docker%E5%9F%BA%E7%A1%80/","categories":[["Docker","/categories/Docker/"]],"content":"Docker基础知识Docker的基本组成 镜像（image)： docker镜像就好比是一个模板，可以通过这个模板来创建容器对外提供服务，如通过tomcat镜像创建一个tomcat容器，容器内的tomcat运行我们的war包对外提供服务，而一个镜像可以创建多个容器，我们不需要像传统部署一样每次都手动部署war，修改端口配置等。 容器(container)： Docker利用容器技术，独立运行一个或者一组应用，容器通过镜像来创建。对容器的基本操作有启动，停止，删除等，可以把这个容器理解为就是一个简易的Linux系统。 仓库(repository)： 仓库就是存放镜像的地方，仓库分为公有仓库和私有仓库，类似GitHub，DockerHub便是镜像仓库。 Docker如何工作Docker是一个Client-Server结构的系统，Docker的守护进程运行在主机上。通过Socket从客户端访问！Docker-Server接收到Docker-Client的指令并执行。 为什么Docker比VM快 docker有着比虚拟机更少的抽象层。由于docker不需要Hypervisor实现硬件资源虚拟化,运行在docker容器上的程序直接使用的都是实际物理机的硬件资源。因此在CPU、内存利用率上docker将会在效率上有明显优势。 docker利用的是宿主机的内核,而不需要Guest OS。 当docker新建一个容器时，docker不需要和虚拟机一样重新加载一个操作系统内核，避免了引导、加载操作系统内核返个比较费时费资源的过程。VM新建一个虚拟机时，虚拟机软件需要加载GuestOS，而docker由于直接利用宿主机的操作系统,则省略了这个复杂的过程 Hypervisor：虚拟机监视器（virtual machine monitor，缩写为 VMM），是用来建立与执行虚拟机器的软件、固件或硬件。 GuestOS： VM（虚拟机）里的的系统（OS） HostOS：物理机里的系统（OS） Docker安装Docker的安装要求Linux在内核3.0以上 官方安装文档： 帮助命令 docker全部命令官方文档： 操作镜像docker images作用：查看镜像 docker images查看当前系统的镜像（默认隐藏中间的镜像） docker images -a查看全部镜像，包括中间层的镜像 docker images -q查看镜像，仅显示镜像id docker images -f &lt;key=valu&gt;查看镜像，根据条件过滤 key的范围： dangling：显示标记为&lt;none&gt;的镜像，取值范围：true | false，如：docker images -f dangling=true label：根据标签进行过滤，其中lable的值，是docker在编译的时候配置的或者在Dockerfile中配置的 before：根据某个镜像的构建时间进行过滤，before的value表示某个镜像构建时间之前的镜像列表，如：docker images -f before=mysql since：跟before正好相反，表示的是在某个镜像构建之后构建的镜像 reference：添加正则进行匹配，如：docker images -f reference=&quot;*:latest&quot;（查询版本为最新版本的镜像） docker search作用：搜索远程仓库的镜像 如：docker search mysql -f STARS=3000（查询远程仓库镜像,查询mysql的镜像，且stars数量大于等于3000） docker pull作用：从远程仓库拉取镜像 格式docker pull &lt;镜像名[:版本]&gt;，等价于docker image pull &lt;镜像名[:版本]&gt;，如docker pull mysql（拉取mysql镜像，默认拉取最新版，latest），docker pull mysql:5.7（拉取mysql5.7镜像） docker rmi作用：删除镜像 格式：docker rmi &lt;镜像名或镜像ID&gt; ，等价于docker image rm &lt;镜像名或镜像ID&gt; 。镜像删除时，docker默认不允许删除正在运行的容器所引用的镜像，除非指定-f选项。 docker rmi mysql（删除mysql的镜像） 删除时可通过镜像名:tag指定版本（标签）。如：docker rmi mysql:5.7这实际上只会删除mysql镜像为5.7的标签，除非该镜像仅有此标签，那么镜像就真的被删除 docker rmi -f mysql：强制删除镜像 docker rmi -f $(docker images -aq) （强制删除全部镜像，先查出id后删除） docker images -q|xargs docker rmi：使用管道符传参进行删除 docker rmi -no-prune：不移除该镜像的过程镜像，默认移除 同一镜像有多个tag情况下，执行 docker rmi &lt;镜像ID&gt; 指令无法删除 操作容器扫盲：容器通过镜像创建而来，镜像与容器为1对多的关系。 docker run作用：新建并运行容器 docker run等价于docker container run 常见命令格式：docker run [option1][option2] &lt;镜像id或名称&gt;，如：docker run --name=mysql-test mysql:5.7（使用mysql5.7镜像新建并运行一个容器，容器名为“mysql-test”）。 docker run 命令支持的option非常多，介绍部分option： –name：指定容器名称 -i：以交互模式运行容器，通常与 -t 同时使用 -t：为容器重新分配一个伪终端，通常与 -i 使用 -d：以后台模式运行容器，并返回容器id，即启动守护式容器 -v：挂载卷（此处不展开讲，见下文） -P：随机映射宿主机一个端口到容器内的一个端口 -p：指定端口映射，模式如下： ip:&lt;宿主机端口&gt;:&lt;容器端口&gt;：宿主机IP+端口映射到容器内端口（如多网卡时） ip::&lt;容器端口&gt;：自动选择宿主机端口（包含宿主机所有IP）映射到容器内端口 &lt;宿主机端口&gt;:&lt;容器端口&gt;：宿主机端口映射到容器内端口 –rm：容器停止时删除容器 docker create作用：新建容器，但不启动 常见命令格式：docker create[option1][option2] &lt;镜像id或名称&gt;，option选项参数含义与run类似但不是全都支持 docker ps作用：查看容器的运行状况，类似linux的ps命令查看进程的运行状态 docker ps等价于docker container list等价于docker container ls docker ps：列出正在运行的容器 docker ps -a：列出全部容器，包括历史记录中已经停止的容器 docker ps -q：列出正在运行的容器，仅显示容器id docker ps -n=?：列出最近创建的?个容器 docker ps -l：列出最近创建的上个容器，即docker ps -n=1 docker stop作用：停止正在运行的容器 常见命令格式：docker stop &lt;容器id或容器名&gt;，如： docker stop mysql-test：停止容器名为mysql-test的容器 docker stop d29f876f66d5：停止容器id为d29f876f66d5的容器 docker kill作用：强制停止正在运行的容器，类似linux的kill命令 常见命令格式：docker kill &lt;容器id或容器名&gt;，如： docker kill mysql-test：强制停止容器名为mysql-test的容器 docker start作用：启动容器 常见命令格式：docker start &lt;容器id或容器名&gt;，如： docker start mysql-test：启动容器名为mysql-test的容器 docker restart作用：重启容器 常见命令格式：docker restart &lt;容器id或容器名&gt;，如： docker restartmysql-test：重启容器名为mysql-test的容器 docker rm作用：删除容器 正常情况下正在运行的容器不可删除。 常见命令格式：docker rm &lt;容器id或容器名&gt;，如： docker rm mysql-test：删除容器名为mysql-test的容器 docker rm -f mysql-test：强制删除容器名为mysql-test的容器，即使该容器正在运行 与容器交互docker attach常见命令格式：docker attach &lt;容器id或容器名&gt;，此命令将进入容器正在执行的终端，如容器启动的前台进程是SpringBoot的jar（以前台模式启动），那么此方式进入容器后看到的就是SpringBoot应用的控制台打印的日志。 docker exec常见命令格式：docker exec [option] &lt;容器id或容器名&gt;，进入当前容器后开启一个新的终端，option选项常用有： -i：即使没有附加也保持STDIN（标准输入）打开 -t：分配一个伪终端 -d：分离模式: 在后台运行 选项意义与run命令的选项类似，如： docker exec -it mysql-test /bin/bash：我们想进入容器，执行任何命令，就像平时使用linux那样，可同时指定-it选项。 docker exec -t mysql-test ls /：我们想查看某一目录下的文件信息，可以仅指定-t选项来进行回显，执行完毕后就退出了容器。 docker exec -i mysql-test /bin/bash：当使用-i选项执行/bin/bash时，由于标准输入打开，我们可以输入其它命令，但是此时是没有回显的。正常情况下只能用Ctr+C结束与之的交互，此时容器内由”/bin/bash”命令创建的进程不会被结束。 docker exec mysql-test ls：不带option，有回显，命令执行完毕后，docker发现没有前台进程，退出容器。 单独使用-i的场景不多，更多使用的是-t和-it。笔者这里对这几个选项的描述可能并不是特别准确，心里明白但无法用文字完美诠释，读者还是自己实操理解会好些。 退出容器 docker attach方式进入容器：按下Ctrl+P+Q退出，如果使用exit退出终端或Ctrl+c方式结束当前前台进程，会导致容器停止！如果使用docker attach --sig-proxy=false &lt;容器id或容器名&gt;进入容器，则可以使用Ctrl+c方式退出 docker exec方式进入容器：按下Ctrl+P+Q退出，或以exit命令退出当前终端以退出容器。 两种进入容器的方式使用Ctrl+P+Q退出时，容器不会停止，因为此种方式会保留进入容器时的执行该命令的进程，容器不会退出，如果一个容器已经后台运行，我们以docker exec -it进入容器时，最好以exit命令退出，容器不会保留终端进程。如果是以docker attach方式进入，此时我们来到了容器守护的前台进程，一定要以Ctrl+P+Q方式退出，否则容器会停止。 容器内外的文件传输docker cp作用：从容器内拷贝文件（文件夹也是文件）到宿主机上 常见命令格式：docker cp [option] &lt;srcPath&gt; &lt;destPath&gt; option选项：-L，以保持源目标中的链接 srcPath与destPath分两种情况： 容器内拷贝文件到容器外：docker cp &lt;容器id或容器名&gt;:&lt;容器内路径&gt; &lt;主机路径&gt; 容器外拷贝文件到容器内：docker cp &lt;主机目的路径&gt; &lt;容器id或容器名&gt;:&lt;容器内路径&gt; ，容器内的路径不存在则创建，即重命名 容器数据卷《Docker容器数据卷》篇 容器内外的网络连通端口映射使用run命令时指定端口映射，前文提到过，见docker run命令的-p与-P选项。 Docker网络《Docker网络》篇 容器的日志docker logs作用：查看容器运行产生的日志 命令格式：docker logs [oprion1][option2] &lt;容器id或容器名&gt; ，常用option： docker logs mysql-test ：查看容器名为mysql-test的日志 docker logs -f mysql-test ：查看且跟踪容器名为mysql-test的日志 docker logs -f mysql-test ：查看且跟踪容器名为mysql-test的日志 docker logs -n=100 mysql-test ：查看容器名为mysql-test的最后100行日志 docker logs --since=2021-02-01 mysql-test：查看容器名为mysql-test自2021年2月1日以来的日志 docker logs --since=2021-02-01T00:00 mysql-test：查看容器名为mysql-test自2021年2月1日00:00以来的日志 docker logs --since=2021-02-01T00:00:00 mysql-test：查看容器名为mysql-test自2021年2月1日00:00:00以来的日志 查看镜像或容器元数据镜像或容器元数据：镜像或容器的详细信息，镜像元数据包括镜像完整id、构建时间等，容器元数据包括数据卷、网络等信息。 docker inspect docker inspect &lt;镜像名或镜像ID&gt;（查看镜像元数据） docker inspect java：查看java（latest版本）的镜像元数据 docker inspect java:7：查看java7的镜像元数据 docker inspect &lt;容器名或容器ID&gt;（查看容器元数据） docker inspect mysql-test：查看容器名为mysql-test的容器元数据 Docker镜像原理镜像是一种轻量级、可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件，它包含运行某个软件所需的所有内容，包括代码、运行时库、环境变量和配置文件。 镜像加载原理UnionFS（联合文件系统）复制粘贴一下概念：Union文件系统（UnionFS）是一种分层、轻量级并且高性能的文件系统，他支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下。Union文件系统是 Docker镜像的基础。镜像可以通过分层来进行继承，基于基础镜像（没有父镜像），可以制作各种具体的应用镜像。特性：一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录。 UFS可以类比git的commit进行理解。 docker的镜像实际上由一层一层的联合文件系统组成。 boots(boot file system）：主要包含 bootloader和 Kernel, bootloader主要是引导加 kernel, Linux刚启动时会加bootfs文件系统，在Docker镜像的最底层是 boots。这一层与我们典型的Linux/Unix系统是一样的，包含boot加載器和内核。当boot加载完成之后整个内核就都在内存中了，此时内存的使用权已由 bootfs转交给内核，此时系统也会卸载bootfs。rootfs（root file system)：在 bootfs之上。包含的就是典型 Linux系统中的/dev,/proc,/bin,/etc等标准目录和文件。 rootfs就是各种不同的操作系统发行版，比如 Ubuntu, Centos等等。 平时我们安装进虚拟机的CentOS都是好几个G，为什么Docker这里才200M？ 对于个精简的OS，rootfs可以很小，只需要包合最基本的命令，工具和程序库就可以了，因为底层直接用宿主机的kernel，自己只需要提供rootfs就可以了。由此可见对于不同的Linux发行版， boots基本是一致的，rootfs会有差別，因此不同的发行版可以公用bootfs。 镜像的分层基于UFS的特点，大多数docker镜像都是由多层镜像“叠加”而来的，所有的 Docker镜像都起始于一个基础镜像层，当进行修改或培加新的内容时，就会在当前镜像层之上，创建新的镜像层，为什么Docker镜像要采用这种分层的结构呢？ 最大的好处，莫过于资源共享了！比如有多个镜像都从相同的Base镜像构建而来，那么宿主机只需在磁盘上保留一份base镜像，同时内存中也只需要加载一份base镜像，这样就可以为所有的容器服务了，而且镜像的每一层都可以被共享。类似于面向对象多态的体现之一：类的继承。 镜像与容器的联系Docker 镜像都是只读的，当容器启动时，一个新的可写层加载到镜像的顶部，这一层就是我们通常说的容器层，容器之下的都叫镜像层。 如果想要保存当前容器的状态，就可以通过docker commit来提交（此处不将commit命令），获得一个镜像，就好比我们我们使用虚拟机的快照。 本文参考B站UP主“狂神说Java”的docker教学视频进行整理。 感谢成长路上为在下传道受业解惑之人"},{"title":"cookie session localStorage sessionStorage","date":"2021-05-12T14:18:29.509Z","url":"/WindShadow/web%E5%9F%BA%E7%A1%80%E6%89%AB%E7%9B%B2/cookie%20session%20localStorage%20sessionStorage/","categories":[["web基础扫盲","/categories/web%E5%9F%BA%E7%A1%80%E6%89%AB%E7%9B%B2/"]],"content":"啰嗦的话不说，直接上干货，概念性的东西，什么产生背景啥的自己组织一下语言就好了 sessionsession是用来在客户端与服务器之间保持状态的一种解决方案（由于http的无状态），主要特点： 数据保存在服务端，session存在过期时间 多数情况下session实现依赖cookie，通过cookie存放sessionId，也可通过URL编码的方式携带sessionId session存放的数据的key不能为null（至少在java web中如此） session存放的数据的value为null时视为删除操作（至少在java web中如此） 放源码，来自apache tomcat的session实现类org.apache.catalina.session.StandardSession cookiecookie大小限制，最多4K。 cookie主要5大属性： name：名称 value：值 domain：作用域，默认情况下，cookie的domain是设置该cookie的web服务器的域名，如domain域为.ccc.com的cookie可以被www.aaa.ccc.com和www.bbb.ccc.com的网站所读取，不能将一个cookie的域设置成服务器所在的域之外的域 ；cookie的domain属性不与端口相关，即同IP不同端口的web服务有机会获取到对方的cookie（path值匹配的话） path：路径值；服务器后端获取cookie的规则：设当前URI为：/a/b，cookie路径为path，服务器后端能获取到的cookie满足：path.startsWith(URI) || URI.startsWith(path) expires/max-age：过期时间/最大“年龄”；根据过期时间 max-age 有3种情况： max-age &gt; 0；cookie到期销毁，此时cookie存于磁盘 max-age = 0；cookie已经过期，此时cookie销毁 max-age &lt; 0；cookie在关闭浏览器后销毁，此时cookie存于内存 另外还有两个属性值得关注： secure：取值范围 true | false，是否仅支持https传输 SameSite：Chrome 51 开始，浏览器的 Cookie 新增加了一个SameSite属性，限制第三方 Cookie，用来防止 CSRF 攻击和用户追踪，该属性不在服务端体现（至少目前java web是这样）。取值范围： Strict：完全禁止第三方 Cookie，任何跨域请求浏览器都不会发送cookie Lax：次于Strict的限制级别，任何跨域大多数情况浏览器不会发送cookie，如下表 请求类型 示例 Lax 链接 &lt;a href=”…”&gt;&lt;/a&gt; 发送 Cookie 预加载 &lt;link rel=”prerender” href=”…”/&gt; 发送 Cookie GET 表单 &lt;form method=”GET” action=”…”&gt; 发送 Cookie POST 表单 &lt;form method=”POST” action=”…”&gt; 不发送 iframe &lt;iframe src=”…”&gt;&lt;/iframe&gt; 不发送 ajax $.get(“…”) 不发送 Image &lt;img src=”…”&gt; 不发送 None：不禁止第三方 Cookie，跨域请求浏览器都会发送cookie，当secure为true时，该值才有效（此条知识点以Chrome浏览器80版本之后为前提） 后端读写cookie后端读取cookie的特点： 后端通过request对象获取cookie：Cookie[] cookies = request.getCookies()，值得注意的是获取到的cookie数量为0时，该方法返回的是null而不是空数组。 设当前访问的地址URI为 /xxx/yyy，写入规则如下： 属性 后端写cookie的值 描述 浏览器实际存储的值 含义 name “abc” “abc” name null null引用 后端抛IllegalArgumentException异常 name “”或” “ 空白字符串 后端抛IllegalArgumentException异常 name “ abc”或“abc ” 空白字符开头或结尾的字符串 后端抛IllegalArgumentException异常 value null null引用 “” value “” 长度为0的字符串 “” value “ “ 空白字符串 后端抛IllegalArgumentException异常 path /xxx 当前路径的或子路径 /xxx path / / 代表根 ${contextPath} web容器上下文 path null null引用 ${contextPath} web容器上下文 path “”或” “ 长度为0或空白字符串 ${contextPath} web容器上下文 path xyz 不以斜杠开头的字符串 ${contextPath} web容器上下文 path /zzz 其它路径 /zzz expires/max-age age &gt; 0 过期时间大于0秒 age * 1000ms cookie到期销毁 expires/max-age age = 0 过期时间等于0秒 0ms cookie已经过期，销毁 expires/max-age age &lt; 0 过期时间小于0秒 N/A(Session) 关闭浏览器时cookie销毁 当${contextPath}web容器上下文为空时，则写入浏览器的path为 /。 前端读写cookie代码可封装如下 当前浏览器地址为 /xxx/yyy 属性 前端写cookie的值 浏览器实际存储的值 后端接受到的值 含义 name “abc” “abc” “abc” name “”或” “ “” ${value} name无效，后端取出的name反而为其value值 name “ abc” “abc” “abc” 前端存cookie会处理name的首尾空白字符 name null “null” “null” null类型转换为”null”字符串 name undefined “undefined” “undefined” undefined类型转换为”undefined”字符串 value “abc” “abc” “abc” value “” “” “” value “ “ “” “” 前端存cookie会处理value的首尾空白字符 value null “null” “null” null类型转换为”null”字符串 value undefined “undefined” “undefined” undefined类型转换为”undefined”字符串 path /xxx /xxx null（后端不感知path值） path /zzz /zzz null（后端不感知path值） 其它路径 path / / null（后端不感知path值） / 根 path xyz /xxx null（后端不感知path值） 当前路径的前一级路径，若不存在则为 / path undefined /xxx null（后端不感知path值） 当前路径的前一级路径，若不存在则为 / path null /xxx null（后端不感知path值） 当前路径的前一级路径，若不存在则为 / path “” /xxx null（后端不感知path值） 当前路径的前一级路径，若不存在则为 / path “ “ /xxx null（后端不感知path值） 当前路径的前一级路径，若不存在则为 / expires “Session” N/A(Session) -1（后端不感知过期时间） 关闭浏览器时cookie销毁（存于内存） expires t &gt; 0 过期时间大于0秒 -1（后端不感知过期时间） cookie到期销毁（存于磁盘） expires t &lt; 0 过期时间小于0秒 -1（后端不感知过期时间） cookie已经过期，销毁 前端js代码写cookie时，过对document.cookie赋值进行设置cookie 设置的属性基本都有默认值，如果设置的值是“无理”的，属性都会取默认值，path属性默认为当前路径的前一级路径，expires属性默认为”Session”。 设置的name和value属性会进行一定优化，需要主要后端取到实际值的不同 localStoragelocalStorage属性在HTML5时增加，允许在浏览器中存储 key/value 对的数据，特点： 只支持string类型的存储，key和value哪怕set时是number类型也会转成string类型 遵循同源策略 永久存在浏览器中，除非手动删除 一般浏览器支持的是5M大小，不同的浏览器中会有所不同 浏览器发送请求时不会带上 sessionStoragesessionStorage属性在HTML5时增加，允许在浏览器中存储 key/value 对的数据，特点： 只支持string类型的存储，key和value哪怕set时是number类型也会转成string类型 数据仅在当前浏览器窗口有效 一般浏览器支持的是5M大小，不同的浏览器中会有所不同 浏览器发送请求时不会带上 小结 数据存放位置 非手动情况下数据销毁时机 session 服务端（依赖cookie存放sessionId） session过期 cookie 浏览器 到达过期时间 localStorage 浏览器 必须手动删除 sessionStorage 浏览器 浏览器窗口关闭 "},{"title":"Nginx使用ssl","date":"2021-03-15T13:31:41.751Z","url":"/WindShadow/Nginx/Nginx%E4%BD%BF%E7%94%A8ssl/","categories":[["Nginx","/categories/Nginx/"]],"content":"Nginx使用sslnginx往往通过反向代理屏蔽服务端，nginx使用ssl的可以让客户端使用https协议与nginx通信，在一定程度上保证整个链路的数据安全。 nginx配置ssl证书与私钥配置相对路径起点为位置为nginx安装目录，假设nginx安装目录下存在ssl文件夹 一般我们还会配置http请求的80端口重定向到443端口 此配置方式为https的单向认证，一般自签证书使用这种方式较多 证书生成在javaweb中使用jks比较多，这里介绍使用keytool生成的jks密钥库应用到nginx的方法，需要结合openssl使用。一个大坑：尽量不要使用windows下git（git bash）自带的openssl，如果执行openssl的命令需要交互式的输入数据（如密钥库口令）时，git bash窗口直接卡住，没有提示也输入不了。 使用假设有ws-ssl-server.jks密钥库存在。 jks密钥库转换成挣p12类型密钥库 使用openss将p12密钥库提取出crt证书 提取私钥 如前文一样，在nginx配置文件中配置ssl证书，nginx -s reload 重新加载即可 双向认证前文介绍的是nginx使用ssl完成https单向认证的操作，此章节介绍双向认证。 ssl常用指令 ssl on | off; 为指定的虚拟主机配置是否启用ssl功能，此功能在1.15.0废弃，使用listen [ssl]替代。 ssl_certificate server.crt; 当前虚拟主机使用使用的证书文件，一般是crt文件 ssl_certificate_key server.key; 当前虚拟主机使用的私钥文件，一般是key文件 ssl_client_certificate ssl client.cer; 客户端证书 ssl_verify_client on; 开启客户端证书验证 ssl_protocols [SSLv2][SSLv3][TLSv1][TLSv1.1][TLSv1.2] 支持ssl协议版本，早期为ssl现在是TSL，默认为后三个 ssl_session_cache off | none | [builtin[:size]][shared:name:size]; 配置ssl缓存：off：关闭缓存；none: 通知客户端支持ssl session cache，但实际不支持； builtin[:size]：使用OpenSSL内建缓存，为每worker进程私有 [shared:name:size]：在各worker之间使用一个共享的缓存，需要定义一个缓存名称和缓存空间大小，一兆可以存储4000个会话信息，多个虚拟主机可以使用相同的缓存名称。 ssl_session_timeout time; 客户端连接可以复用ssl session cache中缓存的有效时长，默认5m（分钟） 关于证书格式：其实证书格式之间的差异几乎也就编码问题，后缀也并不能完全代表什么，每个软件支持的格式可能不一样，如tomcat和nginx的差异，能用就行，不能就转换格式。"},{"title":"Spring框架基础能力-数据转换","date":"2021-02-28T11:48:25.826Z","url":"/WindShadow/Spring/Spring%E6%A1%86%E6%9E%B6%E5%9F%BA%E7%A1%80%E8%83%BD%E5%8A%9B-%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%8D%A2/","categories":[["Spring","/categories/Spring/"]],"content":"Spring的数据转换器众所周知，在书写Spring的配置文件或者前端请求后端时，我们所有配置项的值或参数值都是字符串的形式存在（上传文件的IO流也类似），根据一定的书写规则，Spring可以将这些原本为string类型的值赋值到对应的bean上或SpringMVC控制层的方法的实参上，这得益于Spring中强大的数据转换能力，下面盘点一波“Spring的数据转换器”。 Spring中的数据转换器主要分两大派系： PropertyEditor（属性编辑器） PropertyEditor是JavaBean规范定义的接口，这是java.beans中一个接口，其设计方便对象与String之间的转换工作，而spring将其扩展，方便各种对象与String之间的转换工作。Spring所有的扩展都是通过继承PropertyEditorSupport，因为它只聚焦于转换上，所以只需复写setAsText()、getAsText()以及构造方法即可实现扩展。 Spring 使用PropertyEditor的接口来实现对象和字符串之间的转换，比如将 2020-01-01转化为日期类型等，可以通过注册自定义编辑器来实现此功能。 应用场景： 在基于xml的配置中，我们往往通过字面值为Bean各种类型的属性提供设置值：如double、int类型，在配置文件配置字面值即可。Spring填充Bean属性时如何将这个字面值转换为对应的类型呢？我们可以隐约地感觉到一定有一个转换器在其中起作用，这个转换器就是属性编辑器。 再者便是Spring MVC框架使用多种PropertyEditor分析绑定HTTP请求的各种参数 Converter（转换器） Spring的Converter可以将一种类型转换成另一种类型的一个对象 Spring提供了3种converter接口： Converter接口 ：使用最简单，最不灵活，1:1 ConverterFactory接口 ：使用较复杂，比较灵活 1:N GenericConverter接口 ：使用最复杂，也最灵活 N:N 既然有了PropertyEditor，那为何还需要有Converter呢？因为Java原生的PropertyEditor存在以下两点不足： 只能用于字符串和Java对象的转换，不适用于任意两个Java类型之间的转换； 对源对象及目标对象所在的上下文信息（如注解、所在宿主类的结构等）不敏感，在类型转换时不能利用这些上下文信息实施高级转换逻辑。 鉴于此，Spring 3.0在核心模型中添加了一个通用的类型转换模块。Spring希望用这个类型转换体系替换Java标准的PropertyEditor。但由于历史原因，Spring将同时支持两者。在Bean配置、Spring MVC处理方法入参绑定中使用它们。 **注：如今SpringBoot是开发首先，本文所列罗的源码均来自于SpringBoot 2.3.7.RELEASE ** PropertyEditor属性编辑器PropertyEditor在Bean配置上的使用以字符串转换为自定义对象为需求； 定义一个Student实体类 定义一个Student的属性编辑器，继承PropertyEditorSupport以实现PropertyEditor接口 注入一个属性编辑器的配置 自定义bean组件需要注入Student类的属性，支持校验 配置文件 疑点：Spring如何使用到了我们注册的PropertyEditor？ 因为CustomEditorConfigurer实现了BeanFactoryPostProcessor接口，往beanFactory注册了我们的PropertyEditor PropertyEditor在MVC参数绑定上的使用首先要清楚一个概念，MVC的参数绑定看起来很像bean配置过程，基本也是从字符串到java对象的转换，但是前者是MVC模块的功能，后者是beanFactory的能力，MVC只是Spring体系中的一员，IOC中beanFactory才是整个Spring体系的核心。所以数据转换这样的基础功能，MVC的参数绑定是不能使用beanFactory的转换能力的，因为参数绑定过程不是bean的创建过程，创建的对象不是SpringBean。所以数据转换的功能在MVC模块是需要注册进去才有的，即一次编写，多处注册。 先定义一个控制层的增强 定义控制层 总结PropertyEditor是线程不安全的，一个实例对应一次String转换操作，而且在IOC启动时的bean配置和MVC参数绑定功能上需要各自注册，且MVC参数绑定增强时不能获取参数类型进行按需注册，退而求其次的做法是全部注册可能的PropertyEditor。 Converter转换器Converter在Bean配置上的使用在此之前我们需要了解一个新的接口ConversionService 顾名思义就是Converter的服务，这个接口通过管理Converter、ConverterFactory、GenericConverter统一对外提供转换服务，所以Spring的的Bean转换操作使用的是ConversionService，贴上ConfigurableBeanFactory的接口声明 再看看使用Converter如何实现以字符串转换为自定义对象的需求； 定义实体类Town 定义转换器且加入IOC 往IOC中注入配置类 ConversionServiceFactoryBean类是一个工厂bean 自定义bean组件需要注入Town类的属性，支持校验 配置文件 疑点：为什么我们注册一个到名为”conversionService”类型为ConversionService的bean，Spring IOC容器在bean配置时就可以使用这个conversionService来完成属性的数据转换呢？ 答案在ConfigurableApplicationContext和AbstractApplicationContext的源码里 AbstractApplicationContext在完成beanFactory的初始化工作时，会从beanFactory中获取名为”conversionService”类型为ConversionService的bean，将其作为后续beanFactory的转换服务。 Converter在MVC参数绑定上的使用前面我们已经把TownConverter加入IOC中了，在Controller上我们可以直接享受其转换能力。 定义控制层 疑点：为什么MVC的参数绑定可以直接使用我们加入到IOC的Converter bean的能力而不需要像PropertyEditor那样额外注册呢？ 解答： 首先我们知道WebMvcConfigurer接口是MVC模块的配置接口，其中有一个addFormatters方法，我们可以通过FormatterRegistry注册器注册我们的Formatter（见下文）、Converter等。 FormatterRegistry继承了ConverterRegistry接口 ConverterRegistry接口可以注册Converter等 切入点： 在MVC的自动装配类WebMvcAutoConfiguration中，可以看到这样一段代码 WebMvcAutoConfigurationAdapter实现WebMvcConfigurer接口注册到IOC中，并调用ApplicationConversionService.addBeans(registry, this.beanFactory);方法注册了一些bean 继续跟源码 ApplicationConversionService.addBeans(registry, this.beanFactory)；可以看到beanFactory中的Converter bean被注册到FormatterRegistry中。 同时WebMvcAutoConfigurationAdapter还引入了EnableWebMvcConfiguration配置类 不难发现，MVC的数据转换服务是由名为”mvcConversionService”的ConversionService完成的，看看mvcConversionService这个bean的创建做了哪些事。 进WebConversionService源码 DefaultFormattingConversionService构造器 小结，EnableWebMvcConfiguration配置类提供了MVC的默认配置，并添加注册了框架默认的Converter 到mvcConversionService，利用mvcConversionService完成 RequestMappingHandlerMapping 的配置 如图： 总结Converter加入IOC可以被MVC管理，从而在参数绑定上使用其能力，但是要注意controller方法写法的区别。在Bean配置上使用Converter则需要手动注册到名为”conversionService”类型为ConversionService的配置bean中，IOC在bean配置时才能享受到其能力。 Formatter数据转换中还有一个特殊的接口，Formatter 容易想到格式化器是java对象和String之间的转换功能在不同地区语言上的加强，所以它是在MVC控制层使用的，根据不同地区信息进行数据转换。 Formatter数据转换定义实体类User 定义格式化器，加入IOC 定义controller Formatter view视图格式化在MVC后端视图渲染时，我们可以通过格式化来指定数据在视图中的呈现内容，比如有一个pojo 我们希望，视图中使用NumberWrapper的code属性时，将code数值进行单个拆分，如”123” -&gt; “1-2-3”，将number也进行拆分，并指定分隔符为”=”，如如”123” -&gt; “1=2=3”，分隔符通过NumberSeparate注解指定。 在controller中 show.jsp内容 效果 代码实现 定义注解 实现一个注解格式化工厂AnnotationFormatterFactory，且加入IOC容器 这样就可以在view视图上使用Formatter 的格式化能力 同时在参数绑定上也可以完成字符串到java对象转换 疑点：为什么Formatter在MVC参数绑定上拥有和Converter一样的效果？ 还记得MVC的自动装配中，名为”mvcConversionService”类型为 WebConversionService 的bean吗，其继承关系如图，看的出其继承FormattingConversionService实现了ConversionService 接口 而FormattingConversionService在注册Formatterr时，通过内部类PrinterConverter和ParserConverter进行封装，实际注册的是它俩，所以MVC才拥有了数据转换的能力，故表面上看Formatter拥有和Converter一样的效果，源码如下 总结Formatter用于数据对不同地区的语言信息进行格式化，如MVC视图的数据呈现格式化，同时因为FormattingConversionService内部对Formatter进行了增强，所以Formatter有了数据转换的能力，而且可以根据语言信息进行不同的转换。"},{"title":"SpringBoot常用注解","date":"2021-02-28T11:06:07.934Z","url":"/WindShadow/SpringBoot/SpringBoot%E5%B8%B8%E7%94%A8%E6%B3%A8%E8%A7%A3/","categories":[["SpringBoot","/categories/SpringBoot/"]],"content":"话不多说，过一下配置文件 yml配置文件主要写法参数写法 激活配置文件或配置块 另外，properties文件，特殊#—注释用于标记文档拆分 外部配置加载顺序SpringBoot也可以从以下位置加载配置﹔优先级从高到低﹔高优先级的配置覆盖低优先级的配置，所有的配置会形成互补配置，以下列举实用的外部配置，官网文档 命令行参数，多个参数用空格分开 来自java:comp/env的JNDI属性 Java系统属性( System.getProperties() ) 操作系统环境变量 RandomValuePropertySource配置的random.*属性值 jar包外部的application-{profile}.properties或application.yml(带spring.profile)配置文件 jar包内部的application-{profile}.properties或application.yml(带spring.profile)配置文件 jar包外部的application.properties或application.yml(不带spring.profile)配置文件 jar包内部的application.properties或application.yml(不带spring.profile)配置文件 @Configuration注解类上的@PropertySource 通过SpringApplication.setDefaultProperties指定的默认属性 优先级从 1 &gt; 2 &gt; … &gt; 11，jar包外 &gt; jar包内，带{profile} &gt; 不带{profile} ； @ConfigurationProperties从配置文件给bean配置属性 基本用法搭配组件型注解，如@Component，指定前缀，以属性名进行赋值，依赖于空构造函数； ignoreInvalidFields 当配置文件中的值无法映射给bean的属性。如字符串给数字赋值等，SpringBoot应用会抛异常而起动失败，而ignoreInvalidFields属性设置为true（默认为false）时可以使应用不会停止，即使用属性默认的值。 ignoreUnknownFields 忽略位置属性；当配置文件中出现了类中没有定义的属性时，即出现了未知的属性，SpringBoot默认时忽略它的（ignoreUnknownFields默认为true），ignoreUnknownFields设为false时，表示要对未知属性“斤斤计较”，结果自然是启动失败。 使用 Spring Boot Configuration Processor自动补全pom.xml中加入Spring Boot Configuration Processor的依赖，并重新build之后，IDEA中编写配置文件时就可以自动补全了 标记配置属性为 Deprecated使用@DeprecatedConfigurationProperty注解作用与属性的get方法上，表示该属性已经过时，重新build之后，属性对应的自动补全提示也会发生改变 搭配@ConstructorBinding@ConstructorBinding注解，顾名思义，构造绑定，通过构造方法，从配置文件获取值给bean设置属性。使用此方式时，需要通过@EnableConfigurationProperties注解修饰的配置类来扫描该类加入IOC，不能对通过常规Spring机制创建的bean使用构造函数绑定，即不能添加组件型注解（如@Component）这样的注解加入IOC，原因也很简单，这里不作解释。 搭配@ConstructorBinding的配置方式，一般在给类内为 final的属性初始化。 搭配@Bean搭配@Bean注解时，不依赖该bean的无参构造，仅在bean实例化后进行属性设置。 @ConfigurationPropertiesBinding与自定义转换器实现org.springframework.core.convert.converter.Converter接口实现自定义转换器，使用@ConfigurationPropertiesBinding声名为Spring可用的转换器 设置性别类。不设置枚举是因为Spring默认支持枚举的字符串配置了。 实现性别类的转换器，接收不符合规则的String抛出IllegalArgumentException异常 建立一个pojo yml文件如下配置即可达到目的，配置的值不是 man或woman则抛出异常 一般来说像上面这种情况使用枚举即可，这里为了举例不使用枚举。 条件注入仅列举常用的条件注入（有必要的话，可以取去了解SpringBootCondition） @ConditionalOnProperty根据【配置文件是否存在对应的键值对】作为注入条件 以下配置可理解为：配置文件存在 “ex.config.flag=true”时注入bean，若不存在该key值则默认为false（不注入） @ConditionalOnBean根据【容器内是否存在符合条件的bean】作为注入条件 @ConditionalOnClass根据【工程中引入是否存在该class】作为注入条件 @ConditionalOnJava根据【当前使用的java版本与配置的版本对比结果】作为注入条件 @ConditionalOnExpression根据【spel表达式结果】作为注入条件，注解内值不符合表达式规则则抛出异常 @ConditionalOnMissingBean根据【容器内是否不存在符合条件的bean】作为注入条件 @ConditionalOnMissingClass根据【容器内是否不存在该class的bean】作为注入条件 @ConditionalOnMissingFilterBean根据【容器内是否不存在该过滤器】作为注入条件 @ConditionalOnCloudPlatform根据【应用所处云平台】作为注入条件 @ConditionalOnWebApplication根据【应用是否所处于web环境】作为注入条件 @ConditionalOnNotWebApplication根据【应用是否所不处于web环境】作为注入条件 @ConditionalOnResource根据【资源是否存在】作为注入条件，如日志的相关配置等 @ConditionalOnSingleCandidate根据【容器内是否存在该class的bean，且只有一个实例或为首选的】作为注入条件，即容器中存在该class的bean的实例，且只有一个时注入条件为true，当存在多个实例时，有一个实例为首选的（如加上了@Primary注解），注入条件也为true，即在其它bean中注入该class的bean时，如使用@Autowired且不指定bean名称时，不会发生依赖注入失败，这个时候，以这个calss为value的@ConditionalOnResource注解条件匹配结果就是true。 条件注入的组合组合条件 AND 在类上使用多个@ConditionalOnXxxx 自定义注解上使用其它条件注入注解 应用到要进行条件注入的组件上 继承AllNestedConditions类封装条件，使用静态内部类标注条件，所有条件将进行【逻辑与】操作 其中ConfigurationPhase枚举的两个值意义如下 组合条件 OR 继承AnyNestedCondition类封装条件，使用静态内部类标注条件，所有条件将进行【逻辑或】操作 组合条件 NOT 继承NoneNestedConditions类封装条件，使用静态内部类标注条件，所有条件将进行【逻辑非】操作 Servlet组件的扫描ServletComponentScan在@SpringBootApplication上使用@ServletComponentScan注解后，Servlet、Filter、Listener可以直接通过@WebServlet、@WebFilter、@WebListener注解自动注册，无需其他代码。 "},{"title":"Nginx原理初探","date":"2021-02-28T11:06:07.931Z","url":"/WindShadow/Nginx/Nginx%E5%8E%9F%E7%90%86%E5%88%9D%E6%8E%A2/","categories":[["Nginx","/categories/Nginx/"]],"content":"Nginx 多进程架构Nginx 多进程架构：一个master进程和多个worker进程； master进程：主要负责有轻而巧的工作，通过进程间通信对worker 进程发号施令或是处理来自bash的start,stop,reload等用户指令 worker进程：主要负责重而笨的工作，处理来自客户端的连接等，多个worker共享一个监听套接字； worker进程由master进程fork而来 客户端请求处理机制：请求争抢；客户端请求到达nginx时，多个worker进程进行请求争抢 在Linux下，Nginx 使用 epoll 的 I/O 多路复用模型（常见的/O 多路复用模型包括： select 模型、 poll模型、 epoll 模型） 拓展： 在TCP Socket 服务开发中，多进程或多线程共享监听套接字时会面临“惊群问题”（有兴趣了解一下）； 对于主流的linux版本, accept 阻塞调用,已经不存在惊群问题，也就是说多个进程同时accept 同一个 监听套接字,只有一个进程获的连接； Nginx 在linux系统中使用epoll_wait 非阻塞式的方式，对于epoll_wait 非阻塞式的创建连接方式，依旧存在惊群问题。 多进程架构好处： 热部署，可以使用nginx -s reload热部署 每个woker是独立的进程，如果有其中的一个woker出现问题，其他woker独立的继续进行争抢,实现请求过程,不会造成服务中断 首先，对于每个worker进程来说，独立的进程，不需要加锁，所以省掉了锁带来的开销，同时在编程以及问题查找时，也会方便很多。其次，采用独立的进程，可以让互相之间不会影响，一个进程退出后，其它进程还在工作，服务不会中断，master进程则很快启动新的worker进程。当然，worker进程的异常退出，肯定是程序有bug了，异常退出，会导致当前worker上的所有请求失败，不过不会影响到所有请求，所以降低了风险。 Nginx同redis，类似都采用了io多路复用机制,每个worker都是一个独立的进程，但每个进程里只有一个主线程，通过异步非阻塞的方式来处理请求，即使是千上万个请求也不在话下。每个worker的线程可以把一个cpu的性能发挥到极致。所以 worker数和服务器的cpu数相等是最为适宜的。设置少了会浪费cpu性能，设多了会造成cpu频繁切换上下文带来损耗。 在nginx配置文件全局块中，通过 worker_processes 参数配置worker 进程数，设置为 auto时，nginx则会自动设置与cpu核心数相同的数量worker 进程。 推荐文章初探Nginx架构之进程模型与事件处理机制"},{"title":"SpringBoot使用ssl","date":"2021-02-28T11:06:07.931Z","url":"/WindShadow/SpringBoot/SpringBoot%E4%BD%BF%E7%94%A8ssl/","categories":[["SpringBoot","/categories/SpringBoot/"]],"content":"keytoolJDK自带的keytool是一个证书工具，位于\\bin\\keytool.exe（linux同理），用它可以生成ssl证书。 使用 keytool -help 查看可用命令 使用keytool -command_name -help查看命令用法 生成密钥库密钥库可以当作证书仓库来看 keytool -genkeypair [option]或keytool -genkey [option] 选项： 其中： -storetype 指定仓库类型， JKS、 JCEKS、 PKCS12等，默认JKS -keyalg 指定密钥的算法， RSA、 DSA 等，默认DSA -keysize 指定密钥长度，默认2048 -alias 指定密钥对的别名，该别名是公开的 -keystore 密钥库的路径及名称 例： dname详解 CN：Common Name 公用名称，对于 SSL 证书，一般为网站域名；而对于代码签名证书则为申请单位名称；而对于客户端证书则为证书申请者的姓名 OU：Organization Name 单位名称，对于 SSL 证书，一般为网站域名；而对于代码签名证书则为申请单位名称；而对于客户端单位证书则为证书申请者所在单位名称 O：Organization 组织 L： Locality 所在城市 S：State 所在省份 C：Country 所在国家，只能填代表国家的双字母或地区代码，如中国：CN或086 导出证书 导入证书到密钥库 转换证书格式jks转pkcs12 SpringBoot使用ssl写一个http接口 http单向认证 生成一个密钥库，名为“ws-ssl-server.jks”，以jks为例，springboot不支持pkcs12 将密钥库放到资源文件夹下，并在配置文件中配置ssl 配置文件如下： 测试结果：浏览器警告并显示不安全连接 http双向认证 在单向认证基础上，再创建一个密钥库，名为“ws-ssl-client.jks”，类型为jks；此时拥有两个密钥库（服务端与客户端）“ws-ssl-server.jks”和“ws-ssl-client.jks” 这两个密钥库分别导出一个证书，分别为：“ws-ssl-server.cer”、”ws-ssl-client.cer” 将对方的证书导入到自己的密钥库，以上述文件名为例 将客户端密钥库转为由jks转pkcs12，pkcs12是行业标准格式 此时得到5个文件 将服务端与客户端的密钥库（jks）放到项目中 配置文件如下： 访问接口进行测试，会无法访问 此时给浏览器导入客户端证书，重启浏览器后即可访问成功！ "},{"title":"SpringIOC常用注解和接口","date":"2021-02-15T13:03:17.407Z","url":"/WindShadow/Spring/SpringIOC%E5%B8%B8%E7%94%A8%E6%B3%A8%E8%A7%A3%E5%92%8C%E6%8E%A5%E5%8F%A3/","categories":[["Spring","/categories/Spring/"]],"content":"IOC Bean注入相关前提了解BeanDefinition知识点组件型注解使用@Component作为元注解的注解可理解为组件型注解，如常见的@Configuration、@Controller、@Service等 依赖注入注解@Required（已弃用）用于setter方法注入 @Autowired 根据类型注入，从容器中寻找与该属性class相同的bean进行匹配并注入，如果找到多个则抛出异常，找不到则该属性为null； 提供required属性，表示非必须注入，即容器中找不到bean进行注入则拉倒 支持@Primary @Primary使用 有两个同类型不同名称的bean 由于@Autowired是根据class进行注入的，没有@Primary注解的情况下，此处注入会抛出异常，有了@Primary注解使该bean成为@Autowired的首选。 @Qualifier指定名称注入，搭配@Autowired使用，解决@Autowired多个匹配的选择问题 @Resource（JSR250规范提供）javax的注解，定义了资源相关的信息。在IOC中根据bean名称注入，不支持@Primary，不提供required属性 @Inject（JSR330规范提供）拥有@Autowired的功能，支持@Primary，不提供required属性 &gt;&gt;&gt; 开始进阶 @ComponentScan自定义过滤方式@ComponentScan如果什么都不写，默认 basePackages 为当前类所在包，过滤类型为上述组件型注解的类。 @ComponentScan的其它过滤方式很常见了，不再赘述，使用@ComponentScan的过滤类型中的 CUSTOM 类型自定义过滤方式 自定义FilterType类型过滤器，实现 org.springframework.core.type.filter.TypeFilter 接口 在@Configuration上使用 spring会扫描 basePackages 指定包下的所有类，符合过滤规则的都会加入IOC中 @Conditional 条件注入简单了解：@Conditional在SpringBoot中大量使用，因为SpringBoot父工程集成了很多maven依赖，也就是集成了很多组件，但是子工程不一定能用得到，所以在IOC启动时，通过@Conditional注解进行条件注入，把需要的组件加入IOC中，子工程引入对应的starter时，对应的组件注入条件也就为true，即激活，也符合了SpringBoot特点，开关式的配置（不然你以为application配置文件有个开关图标是干嘛的）。 实现org.springframework.context.annotation.Condition接口 在组件型注解或@Bean上使用 @Import 引入组件支持类型@Import 支持的value为class，支持三种类型： 普通bean，被@Configuration注解作用的类也暂且叫做普通bean，Spring会根据该bean注解及接口等信息进行对应的操作，如遇到@Configuration则进行对应配置，遇到@Aspect时进行AOP代理等等 ImportSelector接口实现类 ImportBeanDefinitionRegistrar接口实现类 ImportSelector接口实现类 ImportBeanDefinitionRegistrar接口实现类 使用方式搭配@Configuration使用 @PropertySource与@PropertySourcesvalue-properties.properties的配置文件：properties配置文件的方式仅支持spel表达式 使用配置文件装载一个bean @ConfigurationProperties（SpringBoot特有）yml配置文件：yml支持松散绑定、支持JSR303数据校验、支持复杂类型填充、不支持spel表达式 Spring在创建带有@Validated注解（见下文）的bean的时候，就会对其属性进行JSR303数据校验。 FactoryBean实现FactoryBean接口成为工厂bean，最终加入Spring IOC容器的bean有两个： bean名称 &amp;myFactoryBean，实例是 MyFactoryBean 工厂bean bean名称 myFactoryBean，但实例是 ExampleBeanD 对象的实例，调用了getObject()方法。可通过其它方式修改bean名称，如@Component的value属性 工厂bean的名称 = 目标bean名称前 + &amp;，通过bean名称获取工厂bean时，只需在bean名称前加“&amp;”； 获取bean时，先获取工厂Bean再调用getObject()方法；bean工厂与bean的作用域相同 Bean初始化 实现InitializingBean, DisposableBean接口（官方不建议，与Spring框架强耦合） @Bean中指定初始化方法与销毁方法（适用于导入第三方库的bean时） 使用@PostConstruct和@PreDestroy标注初始化和销毁方法（推荐） 初始化与销毁优先级当一个bean中同时存在多个初始化方法时，如在@Bean中标注初始化方法与销毁方法、该bean又实现InitializingBean, DisposableBean接口、同时该bean使用了@PostConstruct和@PreDestroy注解标注方法。执行顺序如下： @PostConstruct -&gt; InitializingBean接口 -&gt; @Bean(initMethod=””) -&gt; @PreDestroy -&gt; DisposableBean接口 -&gt; @Bean(destroyMethod=””) 即：注解 &gt; 接口 &gt; @Bean标注 Bean实现xxxAware接口bean实现Spring中xxxAware接口可以拿到对应的Spring组件，如图（仅展示部分） BeanFactoryAwareBeanFactory是顶层接口，功能少，面向Spring框架，不建议使用 ApplicationContextAwareApplicationContext扩展了BeanFactory接口，功能丰富，面向开发者，建议使用； setApplicationContext()方法调用时机：ApplicationContextAware是在spring初始化完bean后才注入上下文的 EmbeddedValueResolverAware提供String 解析器 接口方法调用时机xxxAware接口方法的调用时机：每个Aware接口一般都用对应的xxxProcessor来处理调用该接口bean的方法，如ApplicationContextAware对应ApplicationContextAwareProcessor。可以看到ApplicationContextAwareProcessor实现了BeanPostProcessor接口，即bean后置处理器，也就是在bean即将可用之前，处理一波。其它Aware接口类似。 Spring 钩子BeanFactoryPostProcessor和BeanPostProcessor这两个接口都是初始化bean时对外暴露的入口之一； BeanFactoryPostProcessorbean工厂的bean属性后置处理容器，也就是说，Spring IoC容器允许BeanFactoryPostProcessor在容器实际实例化任何其它的bean之前读取配置元数据，并有可能修改它。 BeanDefinitionRegistryPostProcessorBeanDefinitionRegistryPostProcessor是BeanFactoryPostProcessor的子接口,BeanFactoryPostProcessor的作用是在Spring Bean的定义信息已经加载但还没有初始化的时候执行postProcessBeanFactory()来处理一些额外的逻辑，而BeanDefinitionRegistryPostProcessor的作用是在BeanFactoryPostProcessor增加了一个前置处理，当一个Bean实现了该接口后，始化前先执行该接口的postProcessBeanDefinitionRegistry()方法，然后再执行其父类的方法postProcessBeanFactory()。这样就把一个Spring Bean的初始化周期更加细化，让我们在各个阶段有定制它的可能。 BeanPostProcessorbean属性后置处理器，方法也是见名知意，实现BeanPostProcessor接口可以在Bean(实例化之后)初始化的前后做一些自定义的操作，但是拿到的参数只有BeanDefinition实例和BeanDefinition的名称，也就是无法修改BeanDefinition元数据,这里说的Bean的初始化是： 1）bean实现了InitializingBean接口，对应的方法为afterPropertiesSet 2）在bean定义的时候，通过init-method设置的方法 PS:BeanFactoryPostProcessor回调会先于BeanPostProcessor "},{"title":"SpringMVC进阶使用","date":"2021-02-15T13:03:17.406Z","url":"/WindShadow/Spring/SpringMVC%E8%BF%9B%E9%98%B6%E4%BD%BF%E7%94%A8/","categories":[["Spring","/categories/Spring/"]],"content":"Web MVC相关根据官网文档介绍总结 BindingResultBindingResult作为控制层方法的参数时，它保存了该方法BindingResult之前的参数绑定的结果，和HttpServletRequest对象一样，来自SpringMVC“内部”，暂且称之为“内部参数”，其它需要通过请求参数去转换成java bean的参数称之为“外来参数”。 例：有5个mvc处理器（控制层的方法） 上面的代码中，method0都熟悉了，前端参数直接绑定，找不到的参数则直接为默认值，而当参数转换错误时，即前端传入了非数字的字符串， 而后端要转换成Integer类型，如{id : 123abc}要转换到User对象的id属性，此时mvc会抛出绑定参数相关的异常（BindException ，该异常可获取到BindingResult对象），此时前端便会收到400状态码的应答。 而在bindMethod1中，BindingResult对像则保存了mvc绑定user对象的结果：有无错误、错误的属性field和错误信息message等；因为处理器显式声明了BindingResult对像作为形参，所以上述由绑定异常导致的400错误不会发生，继续进入处理器方法执行业务。 在bindMethod2和bindMethod3中，处理器需要绑定2个bean到实参，经验证执行过程如下： 从左到右先绑定User，若绑定失败则抛出参数绑定相关的异常 绑定User通过，绑定Dept失败，进入控制层方法，BindingResult保存id绑定结果 绑定都通过，进入控制层方法，BindingResult保存Dept绑定结果 故而得出：BindingResult保存的绑定结果是处理器（控制层的方法）最后一个bean参数的绑定结果【一对一绑定】 在integerMethod5中，有一个普通数据类型的形参，如果对其转换失败时，mvc则直接抛出方法参数类型匹配异常（MethodArgumentTypeMismatchException），注意不是绑定异常。 稍加思考不难得出，mvc处理器参数处理的简单流程： 当我们需要关心客户端传入bean参数的对错时，可以在处理器上使用BindingResult对象，不关心则不需要，直接让mvc自动返回400错误。一般情况下，很少使用BindingResult对象，BindingResult更多的是在参数绑定相关异常中作为绑定结果信息的载体，通过BindingResult先打一下mvc的基础。 @ControllerAdvice顾名思义，控制器增强，原理是AOP，结合控制层能干的事和AOP方可理解其作用，不多BB。 @InitBinder参数绑定PropertyEditor先了解这个，PropertyEditor的使用和原理 @InitBinder可以实现参数绑定，将String类型的数据转换成对应的java对象 WebDataBinder实现了PropertyEditorRegistry接口，所以只需要注册CustomDateEditor解析器和自定义的大小数转换器（下见代码）就可以了。 自定义参数转换器 简而言之@InitBinder：控制层方法参数映射，String =&gt; Java Object（一对一） @ControllerAdvice + @InitBinder = 被增强的Controller都拥有共同的数据绑定器 更详细的参数绑定原理的博客： @ExceptionHandler异常处理几种异常处理方式先大致了解一下百度上的3中异常处理方式 可得：大致有以下几种异常处理方式 手动try-catch，自己返回对应视图或数据【原始操作，颗粒细，代码臃肿，分支多】 Controller + @ExceptionHandler【只能在当前控制器中处理异常】 BaseController + @ExceptionHandler + 继承【类似 2，但因为继承，具有统一异常处理能力】 实现 HandlerExceptionResolver 接口【可实现统一异常处理】 @ControllerAdvice + @ExceptionHandler 【AOP原理，可实现统一异常处理】 @ExceptionHandler支持的参数和返回值类型； 意思你在@ExceptionHandler的异常处理方法下的操作，类似在@RequestMapping方法下的操作，但是仅限于文档上指明的方法参数类型和返回值。和@RequestMapping一样，返回String依旧默认是视图名称，方法加上@ResponseBody就是json形式返回。 注意：方法参数为map等具有往视图添加数据的类型时，方法被调用时会报错 优先级既然在SpringMVC中有多种处理异常的方式，那么就存在一个优先级的问题： 当发生异常的时候，SpringMVC会如下处理： （1）SpringMVC会先从配置文件找异常解析器HandlerExceptionResolver （2）如果找到了异常异常解析器，那么接下来就会判断该异常解析器能否处理当前发生的异常 （3）如果可以处理的话，那么就进行处理，然后给前台返回对应的异常视图 （4）如果没有找到对应的异常解析器或者是找到的异常解析器不能处理当前的异常的时候，就看当前的Controller中有没有提供对应的异常处理器，如果提供了就由Controller自己进行处理并返回对应的视图 （5）如果配置文件里面没有定义对应的异常解析器，而当前Controller中也没有定义的话，就看有没有全局ControllerAdvice提供的全局异常处理器，如果没有那么该异常就会被抛出来 @Valid数据校验前提了解：JSR303数据校验 JSR303注解小细节设定一个pojo，下面的字段中，如果前端没有传过来email字段，也就是得到User对象的email属性为null，这是允许的，等同于【选填，格式必须为邮箱】，如果在@Email上再加入@NotNull，即叠加校验，等同于【必填，格式必须为邮箱】，叠加校验校验顺序从上往下 控制层写法参数 + BindingResult对象 @Valid 开启校验功能，紧跟在校验的Bean后添加一个BindingResult，BindingResult会封装前面Bean的校验结果，可见参数校验结果是被视为 参数校验结果的子集。这种处理器的写法缺点很明显，每个需要参数校验的地方都用 BindingResult 获取校验结果。冷门操作。 结合@ExceptionHandler 数据校验不起作用的写法 直接验证基本数据类型是无效的；可以这样理解：@Valid作用于bean上，表示对此bean开启JSR303数据检验，bean中所有被JSR303注解作用的属性都会被检验。 多参数验证顺序 和参数绑定一样，验证顺序从左到右，由此可见参数校验属于参数绑定的子过程。 @Validated数据校验@Valid是java的注解（javax.validation包下），@Validated是Spring的注解，Spring对@Validated的功能支持包含了@Valid的支持，即在Spring环境中，@Valid能干的@Validated都能干，所以在控制层的数据校验时，也可用@Validated代替@Valid。另外Spring对@Validated提供了分组校验 分组校验场景：User bean在新增操作时，id要为空，更新操作时id不能为空。@Valid没有分组功能，@Validated拥有分组功能。 建立分组类 User bean 控制层写法 自定义校验注解实现类似上述@Email @Null的检验注解，以行数据逻辑删除属性为例 行数据类 校验注解 处理检验注解的类，必须实现ConstraintValidator接口 控制层直接使用 自定义校验器定义一个Gender类 实现对应的校验器 控制层注册 @MatrixVariable 矩阵变量 RFC3986定义了在URI中包含name-value的规范 。 使用@MatrixVariable可以方便的进行多条件组合查询 开启矩阵变量功能 xml中 官方例子： handler入参为Map&lt;String,String&gt; 和 Map&lt;String,List 返回结果： handler入参为普通简单类型 返回结果： URL + ? + 参数 对比 2 返回结果： 注意点： 显然的，如url = /matrix/user/userls;id=100;name=ls;age=18;address=nj/com/alibaba;id=1001;address=hz;dept=dept001,dept002。分号开始的地方当作一个矩阵，遇到一些非矩阵符号（如 / 或 ? ，它们是正常url里有特别意义的字符）时，矩阵视为结束。 URL格式 dept=dp01;dept=dp02 等价于 dept=dp01,dp02 /matrix3/user/userls;?car=lbjn 此种格式绑定到 @PathVariable(“user”) String user 参数时，user值为 userls; /matrix3/user/userls;age=18;?car=lbjn 和 /matrix3/user/userls;age=18?car=lbjn 这两种格式的差别在 ? 之前的分号，但是他们都能正常绑定到参数。即 age 值为 18 而不是 18; 根据 1 2可知，如url = /matrix3/user/userls;?car=lbjn处理器有@PathVariable(“user”) String user。矩阵user并不被认为存在，所以矩阵参数开始的字符是分号 ； 。这一点不注意可能会被误导。 @MatrixVariable(pathVar = “user”,name = “name”)中，显然 pathVar 代表参数所在的矩阵，只有一个矩阵时也可省略 @MatrixVariable(required = false)表示此参数可以不存在，即required属性控制参数是否必填 @MatrixVariable(defaultValue = “value”)，通过defaultValue属性表示缺省值 使用@MatrixVariable后，传入分号作为参数值时需要转义编码啥的，具体转义规则啥的看RFC3986来吧，目前百度上也没看到有个完美可行的说法，可能大多数人矩阵变量用的不是也别多，个人感觉挺方便的 @ModelAttribute@ModelAttribute注解用于将方法的参数或方法的返回值绑定到指定的模型属性上，并返回给Web视图。被@ModelAttribute注解注释的方法会在此controller每个handler方法（处理器）执行前被执行，因此对于一个controller映射多个URL的用法来说，要谨慎使用。 例A： 打印结果： 例B 打印结果： 例C 例D @RequestAttribute参照@ModelAttribute的使用，只不过数据放的域不同，此时数据存放于request域，即 request.setAttribute() @SessionAttribute参照@ModelAttribute的使用，只不过数据放的域不同，此时数据存放于session域，即 session.setAttribute()"},{"title":"Nginx初入江湖","date":"2021-02-15T13:03:17.405Z","url":"/WindShadow/Nginx/Nginx%E5%88%9D%E5%85%A5%E6%B1%9F%E6%B9%96/","categories":[["Nginx","/categories/Nginx/"]],"content":"Nginx简单入门软件位置，不同安装方式可能不同，一般如下： docker容器：/usr/sbin/nginx centos：/usr/local/nginx/sbin 配置环境变量即可在任意地方使用nginx命令 常用命令默认已经配置环境变量 启动与关闭 检查配置文件nginx.conf的正确性命令 查看版本号 第三方模块安装主要命令解释查看已经有的模块 nginx第三方模块安装方法 Nginx配置文件nginx配置文件可以引入其它配置文件，即include操作 配置文件位置 docker容器：/etc/nginx/nginx.conf centos：/usr/local/nginx/conf/nginx.conf 配置文件构成官方初始的配置文件 构成如下： 全局块 文件开始到events块之前的内容，配置影响nginx整体运行的指令；如worker_processes是nginx服务器处理并发服务的关键配置，值越大能力越强（还得看硬件配置）； events块events 块涉及的指令主要影响Nginx 服务器与用户的网络连接 http块。 http全局块（server块之前）http 全局块配置的指令包括文件引入、MIME-TYPE定义、日志自定义、连接超时时间、单链接请求数上限等 server块（主要关注此处） 反向代理主要修改Http Server块的配置 负载均衡配置 负载均衡策略： 轮询（默认）;按照时间顺序分配请求，其中一台服务器挂掉时自动剔除 权重 weight；默认权重为 1，权重越高得到的请求越多 IP hash；每个请求按访问ip的hash结果分配,这样每个访客固定访问一个后端服务器，可以解决session的问题。 fair；按后端服务器的响应时间来分配请求，响应时间短的优先分配。 动静分离Nginx动静分离简单来说就是把动态请求跟静态请求分开，不能理解成只是单纯的把动态页面和静态页面物理分离。严格意义上说应该是动态请求跟静态请求分开，可以理解成使用Nginx处理静态页面，Tomcat处理动态页面。动静分离从目前实现角度来讲大致分为两种：一种是纯粹把静态文件独立成单独的域名,放在独立的服务器上,也是目前主流推崇的方案；另外一种方法就是动态跟静态文件混合在一起发布，通过Nginx来分开。 通过location 指定不同的后缀名实现不同的请求转发。通过 expires（过期）参数设置，可以使浏览器缓存过期时间，减少与服务器之前的请求和流量。具体Expires定义:是给一个资源设定一个过期时间,也就是说无需去服务端验证,直接通过浏览器自身确认是否过期即可，所以不会产生额外的流量。此种方法非常适合不经常变动的资源。（如果经常更新的文件，不建议使用expires来缓存），设置3d，表示在这3天之内访问这个URL，发送一个请求，比对服务器该文件最后更新时间没有变化，则不会从服务器抓取，返回状态码304，如果有修改，则直接从服务器重新下载，返回状态码200。 配置 假设有文件 /mypath/image/a.png 访问 192.168.100.100/image 则得到一个类似ftp的页面，列出/mypath/static/image文件夹下的文件（因为配置了 autoindex on;）；访问 192.168.100.100/image/a.png 则得到一张图片，即 /mypath + /image/a.png ； 其它动态请求进行代理配置即可。 Server块location入门匹配类型 = 精准匹配 例 通用匹配 例 正则匹配 例 匹配开头路径 例 匹配后缀 正则匹配注意点： ~ 代表进行正则时区分大小写 ~* 代表进行正则时不区分大小写 优先级： (location = ) &gt; (location /xxx/yyy/zzz) &gt; (location ^~) &gt; (location ~，~*) &gt; (location /起始路径) &gt; (location /) Nginx更进一步Nginx变量Nginx的配置文件使用的就是一门微型的编程语言，变量说白了就是存放“值”的容器。而所谓“值”，在许多编程语言里，既可以是3.14这样的数值，也可以是 hello world 这样的字符串，甚至可以是像数组、哈希表这样的复杂数据结构。然而在 Nginx 配置中，变量只能存放一种类型的值，因为也只存在一种类型的值，那就是字符串。 所有的 Nginx 变量在 Nginx 配置文件中引用时都须带上 $ 前缀，直接把变量嵌入到字符串常量中以构造出新的字符串： 变量插值 关键在于引号 使用花括号 变量生命周期Nginx 变量的创建和赋值操作发生在全然不同的时间阶段。Nginx 变量的创建只能发生在 Nginx 配置加载的时候，或者说 Nginx 启动的时候；而赋值操作则只会发生在请求实际处理的时候。这意味着不创建而直接使用变量会导致启动失败，同时也意味着我们无法在请求处理时动态地创建新的 Nginx 变量。我们无法在请求处理时动态地创建新的 Nginx 变量。 Nginx 变量名的可见范围虽然是整个配置，但每个请求都有所有变量的独立副本，或者说都有各变量用来存放值的容器的独立副本，彼此互不干扰。比如前面我们请求了/bar接口后，$foo变量被赋予了值32，但它丝毫不会影响后续对/foo接口的请求所对应的foo值（它仍然是空的！），因为各个请求都有自己独立的foo变量的副本。 Nginx 变量理解成某种在请求之间全局共享的东西，或者说“全局变量”。而事实上，Nginx 变量的生命期是不可能跨越请求边界的 简而言之，nginx的全局变量生命周期是和请求的生命周期一致，每个子请求有自己的全局变量。 Nginx内置变量在配置基于nginx服务器的网站时，必然会用到 nginx内置变量。内置变量存放在 ngx_http_core_module 模块中，变量的命名方式和apache 服务器变量是一致的。总而言之，这些变量代表着客户端请求头的内容，例如$http_user_agent, $http_cookie, 等等。下面是nginx支持的所有内置变量： 常见模块geo模块根据客户端地址创建新变量，常见写法 location中使用 注意点 如果geo指令后不输入address，那么默认就使用变量remite_addr作为ip地址 {} 内的指令匹配：优先最长匹配 map模块 ngx_http_map_module ；默认编译进Nginx ；通过–without-http_map_module禁用； 基于已有变量，使用类似switch{case: … default: …}的语法创建新变量，为其他基于变量值实现功能的模块提供更多的可能性。 简单地说，map 的主要作用是创建自定义变量，通过使用 nginx 的内置变量，去匹配某些特定规则，如果匹配成功则设置某个值给自定义变量。 而这个自定义变量又可以作于他用。 作用域： http 块 三个参数（指令）： default ： 指定源变量匹配不到任何表达式时将使用的默认值。当没有设置 default，将会用一个空的字符串作为默认的结果。 hostnames ： 允许用前缀或者后缀掩码指定域名作为源变量值。这个参数必须写在值映射列表的最前面。 include ： 包含一个或多个含有映射值的文件。 case规则： 字符串严格匹配 使用hostnames指令，可以对域名使用前缀泛域名匹配 使用hostnames指令，可以对域名使用后缀泛域名匹配 ～和～*正则表达式匹配，后者忽略大小写 default规则： 没有匹配到任何规则时，使用default，缺失default时，返回空字符串给新变量 map_hash_bucket_size size;指令： 指定一个映射表中的变量在哈希表中的最大值，这个值取决于处理器的缓存。 根据主机名称（请求的域名）给 $name赋值 实用操作之一，重定向处理 headers_more模块 headers_more模块用于 添加、修改或清除 请求/响应头，该模块不是nginx自带的，需要另外安装。openresty默认包含了该模块，可以直接使用。 headers_more模块主要有4个指令，指令可使用-s选项指定HTTP状态码： more_set_headers 用于修改响应头，具有相同名称的响应头总是覆盖 ，因为set的特性，故也可达到添加、清除响应头的作用； 配置段 语法 more_set_headers [-t &lt;content-type list&gt;]... [-s &lt;status-code list&gt;]... &lt;new-header&gt; 新的响应体格式可如下： name: vlaue name: name注意：more_set_headers允许在location的if块中，但不允许在server的if块中。下面的配置就报语法错误 more_clear_headers 用于清除响应头： 配置段 语法 more_clear_headers [-t &lt;content-type list&gt;]... [-s &lt;status-code list&gt;]... &lt;new-header&gt; more_set_input_headers 用于修改请求头，因为set的特性，故也可达到添加、清除请求头的作用； 配置段 语法 more_set_input_headers [-r][-t ]... &lt;new-header&gt; 类似于more_set_headers，不同之处在于它对输入标头（或请求标头）进行操作，并且仅支持该-t选项。请注意，-t 选项根据请求内容的类型（ Content-Type ）过滤。 more_clear_input_headers 用于清除请求头 配置段 语法 more_clear_input_headers [-t &lt;content-type list&gt;]... &lt;new-header&gt; sub模块ngx_http_sub_filter_module模块，默认未编译进Nginx ,通过–with-http_sub_module启用； sub模块用于响应内容的替换，即替换响应中的字符串；指令如下： sub_filter匹配文本后替换 sub_filter_last_modified是否阻止response header中写入Last-Modified，防止缓存，默认是off，即防止缓存 sub_filter_oncesub_filter指令是执行一次，还是重复执行，默认是只执行一次 ；即替换一次（on），全部替换（off） sub_filter_types指定类型的MINE TYPE才有效 指令set指令set 指令是用于定义一个变量，并且赋值 例： return 指令return 指令用于返回状态码给客户端 例： if指令if指令用来判断条件表达式的结果，为true时执行的指令，条件为false时不执行相应的指令 语法格式与编程语言类似if (condition) &#123; ... &#125; condition可以是如下类型： 变量名，如果变量的值是空字符串或者0表示false 变量使用“=”和“!=”来跟字符串比较 可以是正则表达式 检查文件是否存在使用“-f” 和 “!-f” 检查目录是否存在使用 “-d” 和 “!-d” 检查文件、目录、符号链接是否存在使用 “-e” 和 “!-e” 检查是否是可执行文件使用“-x” 和 “!-x” if指令不支持多条件、不支持嵌套、不支持else，与常见的条件表达式不同的特点是，if指令使用单个等号＝而不是双等号==表达等值比较。 推荐文章狂神的nginx入门"},{"title":"Location通用匹配的注意点","date":"2021-02-15T13:03:17.404Z","url":"/WindShadow/Nginx/NginxLocation/","categories":[["Nginx","/categories/Nginx/"]],"content":"前提应知： 先说一下ContextPath：称为web应用的上下文路径，简单理解为web容器最开始路径，所有的web资源（路径）都要挂在它的下面，使用request.getContextPath()可获得，SpringBoot中的配置为server.servlet.context-path，ContextPath的值只能为以下两种形式： 空字符串：即长度为0的字符串 /xxx：形如“/xxx”形式的字符串，可以是 /xxx、/xxx/yyy、/xx x/yyy，但 /xxx/在SpringBoot中最终会被处理成/xxx 在nginx反向代理的location中，我们都知道location的匹配格式（写法）决定了不同的匹配方式，从而影响最终的代理到的url。通用匹配是最常用的写法，比如为了方便管理，需要做一套应用的代理。一套应用意思就是几个项目互相依赖组成一个可用的应用，如分布式架构、sso等。这个时候我们需要将这些应用都挂到一个nginx（nginx服务器）的端口下，这样对用户来说好像就一直在访问一个应用。对外提供接口也比较方便，因为用的都是同一个端口。 在通用匹配中location和proxy_pass指令的写法还是有必要注意一下ContextPath的值的，避免掉坑。 都知道通用匹配形如 此时若该location块所在的server块监听的端口为8000，那么上述写法可以这样理解：一个代理挂在了8000端口下的，名为 /xxx；因为假如现在需要在此时可以再加个location去代理其它的地址，端口依旧可以用8000,。 若location如下， 则可以这样理解：一个代理独占了8000端口；再加一个location去代理其它地址就不太好操作，搞不好容易出现代理错误的情况。 下面以上述两种location写法做个测试。 通用匹配测试nginx服务器： IP：192.168.100.100 SpringBoot应用： IP：192.168.0.200； 一个控制层方法： 配置文件aplication.properties： 测试结果： context-path location proxy_pass 结果 描述 - / 192.168.0.200:8080 ok - / 192.168.0.200:8080/ ok - /app 192.168.0.200:8080 404 nginx最终代理到：192.168.0.200:8080/app/test - /app 192.168.0.200:8080/ ok /demo / 192.168.0.200:8080/demo 404 nginx最终代理到：192.168.0.200:8080/demotest /demo / 192.168.0.200:8080/demo/ ok /demo /app 192.168.0.200:8080/demo ok /demo /app 192.168.0.200:8080/demo/ ok 可见，location的写法和要代理到的目标地址还是有讲究的，详细的话就不说了，为了防止出错，简单一句话总结： proxy_pass = ip + port + ContextPath，要到的目标应用ContextPath为空字符串时，location写“/”，反之写ContextPath； 有域名的情况自行适配即可。"},{"title":"Git命令大全","date":"2021-02-15T13:03:17.403Z","url":"/WindShadow/Git/git%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8/","categories":[["Git","/categories/Git/"]],"content":"Git命令大全git config配置 Git 的相关参数。 Git 一共有3个配置文件： 仓库级的配置文件：在仓库的 .git/.gitconfig，该配置文件只对所在的仓库有效。 全局配置文件：Mac 系统在 ~/.gitconfig，Windows 系统在 C:\\Users\\&lt;用户名&gt;\\.gitconfig。 系统级的配置文件：在 Git 的安装目录下（Mac 系统下安装目录在 /usr/local/git）的 etc 文件夹中的 gitconfig。 git clone从远程仓库克隆一个版本库到本地。 git init初始化项目所在目录，初始化后会在当前目录下出现一个名为 .git 的目录。 git status查看本地仓库的状态。 git remote操作远程库。 git branch操作 Git 的分支命令。 git checkout检出命令，用于创建、切换分支等。 git cherry-pick把已经提交的记录合并到当前分支。 git add把要提交的文件的信息添加到暂存区中。当使用 git commit 时，将依据暂存区中的内容来进行文件的提交。 git commit将暂存区中的文件提交到本地仓库中。 git fetch从远程仓库获取最新的版本到本地的 tmp 分支上。 git merge合并分支。 git diff比较版本之间的差异。 git pull从远程仓库获取最新版本并合并到本地。 首先会执行 git fetch，然后执行 git merge，把获取的分支的 HEAD 合并到当前分支。 git push把本地仓库的提交推送到远程仓库。 git log显示提交的记录。 git rebase 对远程分支的rebase是指，将自己的提交追加在别人的提交之后，如果自己的提交和远程的提交有相同的文件的改动，远程提交有可能被刷掉 rebase的目的是打开提交的历史，git会让你在一个新的分支修改内容，git rebase -continue则是让你回到之前的版本 对本地分支的rebase 是指将自己本地分支的所有提交合并成为一个，可解决多次小改动的重复提交等 git reset还原提交记录。 git revert生成一个新的提交来撤销某次提交，此次提交之前的所有提交都会被保留。 git stash git tag操作标签的命令。 git mv重命名文件或者文件夹。 git rm删除文件或者文件夹。 Git操作场景示例1. 删除掉本地不存在的远程分支多人合作开发时，如果远程的分支被其他开发删除掉，在本地执行 git branch --all 依然会显示该远程分支，可使用下列的命令进行删除： 2.git stash解决冲突使用IDEA的Stash Changes 和 unStash Changes 来解决冲突。 Stash Changes ：作用：本地的全部改动临时保存到本地仓库，并撤销了本地的所有改动stash的好处是，可以先将你的改动暂存到本地仓库中，随时可以取出来再用，但是不用担心下次push到服务器时，把不想提交的改动也push到服务器上，因为Stash Changes的内容不参与commit和push。 unStash Changes作用：将之前保存得临时改动，取出并合并带本地。 步骤： 我们选择项目右键–Git–Repository–Stash Changes stash完后你会发现你本地进行的一些修改都已经不存在了，这样我们就可以和远程仓库合并了，冲突先去掉了，git pull 顺利成功。 我们选择项目右键–Git–Repository–pull将线上得代码拉取到本地 现在我们再把之前保存的更改取出来 选择项目右键–Git–Repository–UnStash Changes，手动解决冲突 此操作比较实用。"},{"title":"Feign声明式服务调用入门","date":"2021-02-15T13:03:17.402Z","url":"/WindShadow/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8E%E5%BE%AE%E6%9C%8D%E5%8A%A1/Feign%E5%85%A5%E9%97%A8/","categories":[["分布式与微服务","/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8E%E5%BE%AE%E6%9C%8D%E5%8A%A1/"]],"content":"小声bb + 一些吐槽下面先bb一些概念性的东西，虽然部分是copy的，但是整理到一起看起来知识点不是那么散，那些上来就哔哔怎么用怎用的博客文章是真没意思，关键写的还不咋的，要么就说的乱七八糟除了他自己没人看得懂，要么就依赖一大堆，搞个Feign的demo还扯那微服务的东西一大堆，依赖Eurake和微服务环境，单独使用不行？除了cv多少带点思考吧。 相信能看到这篇文章的看客们（可能并没有），多多少少听过微服务的一些概念，也知道Feign能干啥，虽然我也不是特别懂，但我还是会把知识点发散开来，本文就当做一篇随时可以巩固的文章，新手也可通过本文入门。 啥是“声明式服务调用”可以从编程范式入手了解，常见的编程范式有： 命令式编程（Imperative Programming） 声明式编程（Declarative Programming） 函数式编程（Funational Programming） 面向对象编程（Object-oriented Programming） &gt;&gt;&gt; 声明式编程范式：声明式编程表明想要实现什么目的，应该做什么，但是不指定具体怎么做。 这里有一篇笔者认为关于编程范式的不错的文章，可以拓展一下眼界：【CSDN】编程范式（author：一位Python高级开发工程师） 那么我们简单理解一下声明式服务调用：声明调用的URL地址，请求方式,和返回结果，但具体如何调用交给底层实现.为什么要使用声明式服务调用? 对系统使用方，通过设计声明式的接口，开发者无需关心底层实现，而更多的关注上层业务 对系统实现方，通过声明式的接口，上层使用者接口相对稳定前提下，系统可以不断的迭代优化 对整个系统而言，能够更系统的收集更多信息，能够依据策略进行系统行为优化，提升系统效率 使用Feign声明式web客户端，只需要声明一个接口即可，不需要关心传参、发送请求、获取响应内容、关闭连接等细节，Feign全部帮我们做好了。 SpringCloud集成了Feign组件，一个是 spring-cloud-starter-feign，一个是spring-cloud-starter-openfeign，前者已经过时。openfeign使得SpringCloud服务间调用变得更简单方便，但不是有SpringCloud才有Feign。 demo这里我们以SpringBoot环境为例，使用SpringCloud的openfeign组件，哎，但是我们不是微服务环境，使用openfeign的原因是让系统配置少些，这段的主要目的是了解Feign的使用而不是配置。 maven关键配置首先，maven依赖与版本，SpringBoot和SpringCloud版本定义如下 openfeign的依赖不需要写版本 依赖管理块配置如下 Feign的服务提供与消费idea中创建两个model，一个是普通的web项目，做服务提供者（下称提供端），一个做Feign的消费者（下称消费端）。 提供端provider模块配置： 配置文件如下，启动类正常的默认配置即可。 消费端consumer模块配置： 配置文件省略，仅使用测试类测试service的bean即可。我们需要用到两个注解： @EnableFeignClients开启Feign客户端的功能 启动类加上@EnableFeignClients注解 @FeignClient使一个接口成为Feign的客户端 编码测试为了能更好的展现出Feign的能力，服务提供可以大致分为几种情况进行测试： 返回值类型不同：基本类型（String、java基本数据类型的包装归为一类）、自定义引用类型（bean）、泛型等，见下文 参请求数位置不同：控制层方法常见的接收参数情况，请求头、请求体，url等等 出错情况：服务提供端响应为500或404或400等 其它非正常使用情况 先构建pojo类，提供端与消费端都要构建。 自定义引用类型 User 泛型，以常见的REST业务响应格式为例 case1：返回值类型不同1.1 基本类型、bean、泛型，JSONObejct、ResponseEntity（SpringWeb提供的响应体封装）提供端web 消费端service 消费端测试 结果自然是ok的，图就不贴了。 1.2 集合类型提供端 消费端测试 测试结果： 可见接收类型为map类型时，键类型为bean是不可以的； case2：参数位置不同提供端web 消费端service 消费端测试 测试结果ok，图就不贴了。 case3：出错情况提供端web 消费端service 消费端测试 测试结果 提供端运行时异常 提供端检查型异常（500） 提供端参数绑定异常（500） 请求不存在的地址（404） 可以看到提供端出现异常时，消费端这边会抛出FeignException的子类异常，异常信息包含了响应码，FeignException的体系如下（部分子类） case4：其它情况4.1 消费端以Object接收设置新的pojo，内置属性为Object超类 提供端web 消费端service 消费端测试 测试结果 以Objec接收String类型时出错； 以Objec接收List、Set类型时，其底层为ArrayList； 以Objec接收Map和其它类型bean时类型时，其底层为LinkedHashMap； 小结 在接口上使用@FeignClient注解即可配置一个Feign的web客户端，接口方法的书写方式可以同控制层写法类似，支持泛型 成为Feign的web客户端的接口的方法，在调用时，若服务提供端出现异常，即响应错误类型的响应码，如500、400等，该方法会抛出FeignException，该异常为运行时异常。 Feign底层使用json序列化传输，使用LinkedHashMap和ArrayList进行存储，上述测试出错的方法张，仔细分析就能发现这些数据是没办法进行反序列化成对于的数据类型的 Feign客户端的写法SpringMVC式写法通过demo我们知道作为Feign客户端的接口的方法写法可以和SpringMVC控制层的方法类似，即前者是后者的子集，故而也支持使用REST风格的注解，如@GetMapping等等。服务消费端SpringMVC式写法的传参和服务提供端的取参自然是对应的，不过有一个不一样的地方就是，消费端接口上不能使用@RequestMapping注解来声明前缀，因为@FeignClient注解的 path属性就已经做到了 Feign自带写法Feign客户单除了mvc式的写法外，还有Feign自带的一种写法，使用@RequestLine注解，注解上使用表达式来声明请求的方式和地址 想要开启这种写法需要加一个配置类，往IOC中加入类型为Contract的bean，该类的实例决定了Feign客户端的写法，Feign默认的写法是“mvc式”，加上下面这个配置后即可使用Feign自带的写法，即使用@RequestLine注解 不仅如此，@FeignClient与配置类的搭配也有讲究，分两种： 配置类加@Configuration注解，则Feign客户端接口使用@FeignClient可以和“mvc式”一样不加其它配置 配置类不加@Configuration注解，则Feign客户端接口使用@FeignClient需要指定配置类 若使用了【1】的配置，即全局配置，则所以的Feign客户端（接口）都必须使用Feign自带的写法。"},{"title":"URL、URI、ServletPath、ContextPath、RealPath","date":"2021-02-15T13:03:17.401Z","url":"/WindShadow/web%E5%9F%BA%E7%A1%80%E6%89%AB%E7%9B%B2/diffLink/","categories":[["web基础扫盲","/categories/web%E5%9F%BA%E7%A1%80%E6%89%AB%E7%9B%B2/"]],"content":"从request请求可以获取URL、URI、ServletPath、ContextPath、RealPath，它们之间有何区别？Let’s go! 可能的误区以SpringBoot配置为例，spring.application.name表示这个Spring应用叫什么，并不代表这个web应用叫什么，决定web应用叫什么的是server.servlet.context-path，也就是我们常说的应用名或者应用前缀或web容器上下文路径。下面这个配置访问应用的所有地址都是以  开头，而不是  ；而Spring应用名称spring.application.name应用之一便是在SpringCloud微服务框架下服务的标识。 实操验证以SpringBoot环境为例，本地部署，有如下配置： application. properties 一个控制层 结果如下： 描述 值 application. properties server.servlet.context-path=/demo 访问链接  URL  URI /demo/test ServletPath /test ContextPath /demo RealPath C:\\Users\\WS\\AppData\\Local\\Temp\\tomcat-docbase.6884443025746620464.8082\\ 结合表格结果，加入官方术语简单直白的总结一波： URL：即 Uniform Resource Locator（统一资源定位符），资源在网络中的唯一的地址，格式：协议+主机+端口+路径 URI：即 Uniform Resource Identifier（统一资源标识符），用于标识某一互联网资源名称的字符串，格式：路径 ServletPath：Servlet路径，简单理解为web应用提供资源的路径，其表现之一在SpringMVC中就是控制层的一个完整资源路径 ContextPath：web应用的上下文路径，简单理解为web容器最开始路径，所有的web资源（路径）都要挂在它的下面 RealPath：真实路径，简单理解为web资源在服务器的真实物理路径，.HttpServletRequest#getRealPath(String)的方法返回的是web容器所在的真实物理地址（也就是根目录）加上该方法的参数的值的结果，说白了算是字符串拼接，没有验证资源是不是存在。使用场景笔者并不是太了解，可能在某个静态资源定位上比较有用吧，在 Servlet 2.1之后该方法已经过期，替代者如下 ContextPath斜杠“/”问题现在我们知道application. properties 中server.servlet.context-path配置的 “/demo” 就是 ContextPath的值了，发散一下，如果我们不配置，那ContextPath的值是多少呢？答案是空字符串（长度为0），如果配置了“/”或“/demo/”或“//”呢？结果分别是空字符串和“/demo”和启动异常，话不多说直接看源码，进对应配置类 org.springframework.boot.autoconfigure.web.ServerProperties 虽然该配置类是SpringBoot下的，我们知道SpringBoot内置tomcat，故而对外提供web容器（tomcat）的配置，最后转换成web容器对应的配置，所以这个配置类的参数规范肯定是要符合tomcat的要求的。 通过源码不难看出，SpringBoot对server.servlet.context-path进行了处理： 两边去空格 包含有效字符串的情况下，去除末尾的“/”字符 所以配置“/”或“/demo/”的结果也就解释得通了，而配置两个斜杠“//”的情况下，上述代码得到的结果为“/”，这个配置扔到tomcat肯定是报错了，这也就反映出为什么我们在写@RequestMapping的时候要以“/”开头了，除了协议与主机名之间的分隔之外，一个url不应该以双斜杠“//”出现，大多数情况下多斜杠的url是被允许的，它们只会被当做一个斜杠处理，这是web容器对我们的“宽容”，但开发者不应该乱写，更不能只知其表不知其里。"}]